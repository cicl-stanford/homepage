+++
# 0 -> 'Forthcoming',
# 1 -> 'Preprint',
# 2 -> 'Journal',
# 3 -> 'Conference Proceedings',
# 4 -> 'Book chapter',
# 5 -> 'Thesis'

title = "Chain versus common cause: Biased causal strength judgments in humans and large language models"
date = "2024-05-11"
authors = ["A. Keshmirian","M. Willig","B. Hemmatian","U. Hahn","K. Kersting","T. Gerstenberg"]
publication_types = ["3"]
publication_short = "_Proceedings of the 46th Annual Conference of the Cognitive Science Society_"
publication = "Keshmirian, A., Willig, M., Hemmatian, B., Hahn, U., Kersting, K., Gerstenberg, T. (2024). Chain versus common cause: Biased causal strength judgments in humans and large language models. In _Proceedings of the 46th Annual Conference of the Cognitive Science Society_."
abstract = "Causal reasoning is important for humans and artificial intelligence (AI). Causal Bayesian Networks (CBNs) model causal relationships using directed links between nodes in a network. Deviations from their edicts result in biased judgments. This study explores one such bias by examining two structures in CBNs: canonical Chain (A→C→B) and Common Cause (A←C→B) networks. In these structures, if C is known, the probability of the outcome (B) is normatively independent of the initial cause (A). But humans often ignore this independence. We tested mutually exclusive predictions of three theories that could account for this bias (N=300). Our results show that humans perceive causes in Chain structures as significantly stronger, supporting only one of the hypotheses. The increased perceived causal power might reflect a view of intermediate causes as more reflective of reliable mechanisms. The bias may stem either from our interventions in the world or how we talk about causality with others. LLMs are primarily trained on language data. Therefore, examining whether they exhibit similar biases can determine the extent to which language is the vehicle of such causal biases, with implications for whether LLMs can abstract causal principles. We therefore, subjected three LLMs, GPT3.5-Turbo, GPT4, and Luminous Supreme Control, to the same queries as our human subjects, adjusting a key `temperature' hyperparameter. We show that at greater randomness levels, LLMs exhibit a similar bias, suggesting it is supported by language use. The absence of item effects suggests a degree of causal principle abstraction in LLMs."
image_preview = ""
selected = false
projects = []
url_pdf = "papers/keshmirian2024chain.pdf"
url_preprint = ""
url_code = ""
url_dataset = ""
url_slides = ""
url_video = ""
url_poster = "posters/keshmirian2024chain-poster.pdf"
url_source = ""
url_custom = [{name = "OSF", url = "https://osf.io/qaydt/?view_only=c9695fd480874ba9bf15cc6199dc4338"}]
math = true
highlight = true
[header]
# image = "publications/keshmirian2024chain.png"
caption = ""
+++