%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tobias Gerstenberg at 2025-07-22 09:09:35 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{wong2025modeling,
	abstract = {When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) --using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for humanlike, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chainof-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.},
	author = {Wong, Lionel and Collins, Katherine M. and Ying, Lance and Zhang, Cedegao E. and Weller, Adrian and Gerstenberg, Tobias and O'Donnell, Timothy and Lew, Alexander K. and Andreas, Jacob D. and Tenenbaum, Joshua B. and Brooke-Wilson, Tyler},
	date-added = {2025-07-22 09:03:11 -0700},
	date-modified = {2025-07-22 09:09:34 -0700},
	journal = {arXiv},
	title = {Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models},
	url = {http://arxiv.org/abs/2507.12547},
	year = {2025},
	bdsk-url-1 = {http://arxiv.org/abs/2507.12547}}

@article{kirfel2025framework,
	abstract = {Willful ignorance---the deliberate avoidance of knowledge---has profound implications for moral responsibility. Avoiding information about the consequences of one's actions challenges philosophical accounts of responsibility and legal culpability, raising questions about whether it should be treated like ordinary ignorance. Willful ignorance has recently attracted attention from psychology, particularly concerning how people attribute blame in such cases. In this paper, we review how people blame willfully ignorant agents and provide a theoretical framework that outlines several routes along which willful ignorance impacts blame. We propose three explanatory mechanisms for blame attributions to willful ignorance---epistemic, counterfactual, and personal inferences---review supporting evidence for these factors, and identify avenues for future research.},
	author = {Kirfel, Lara and Gerstenberg, Tobias and Zultan, Ro'i},
	date-added = {2025-07-03 12:42:27 -0700},
	date-modified = {2025-07-03 12:42:27 -0700},
	journal = {Current Opinion in Psychology},
	pages = {102090},
	title = {A framework for blaming willful ignorance},
	year = {2025}}

@article{cross2025understanding,
	abstract = {How do we predict others from patterns in their behavior and what are the computational constraints that limit this ability? We investigate these questions by modeling human behavior over repeated games of rock, paper, scissors from Brockbank \& Vul (2024). Against algorithmic opponents that varied in strategic sophistication, people readily exploit simple transition patterns (e.g., consistently playing rock after paper) but struggle to detect more complex sequential dependencies. To understand the cognitive mechanisms underlying these abilities and their limitations, we deploy Hypothetical Minds (HM), a large language model-based agent that generates and tests hypotheses about opponent strategies, as a cognitive model of this behavior (Cross et al., 2024). We show that when applied to the same experimental conditions, HM closely mirrors human performance patterns, succeeding and failing in similar ways. To better understand the source of HM's failures and whether people might face similar cognitive bottlenecks in this context, we performed a series of ablations and augmentations targeting different components of the system. When provided with natural language descriptions of the opponents' strategies, HM successfully exploited 6/7 bot opponents with win rates {\textgreater}80\% suggesting that accurate hypothesis generation is the primary cognitive bottleneck in this task. Further, by systematically manipulating the model's hypotheses through pedagogically-inspired interventions, we find that the model substantially updates its causal understanding of opponent behavior, revealing how model-based analyses can produce testable hypotheses about human cognition.},
	author = {Cross, Logan and Brockbank, Erik and Gerstenberg, Tobias and Fan, Judith E and Yamins, Daniel L K and Haber, Nick},
	date-added = {2025-06-13 22:04:47 -0700},
	date-modified = {2025-06-13 22:04:50 -0700},
	journal = {Computational Cognitive Neuroscience},
	title = {{Understanding human limits in pattern recognition: A computational model of sequential reasoning in rock, paper, scissors}},
	year = {2025}}

@inproceedings{pares-morlans2025pik,
	abstract = {Tasks that involve complex interactions between objects with unknown dynamics make planning before execution difficult. These tasks require agents to iteratively improve their actions after actively exploring causes and effects in the environment. For these type of tasks, we propose Causal-PIK, a method that leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel to help guide efficient search for the best next action. Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. We also compare CausalPIK to human studies, including results from a new user study we conducted on the PHYRE benchmark. We find that Causal-PIK remains competitive on tasks that are very challenging, even for human problem-solvers.},
	annote = {Comment: Accepted to ICML 2025},
	author = {Par{\'e}s-Morlans, Carlota and Yi, Michelle and Chen, Claire and Wu, Sarah A. and Antonova, Rika and Gerstenberg, Tobias and Bohg, Jeannette},
	booktitle = {International Conference on Machine Learning},
	date-added = {2025-05-29 20:43:13 -0700},
	date-modified = {2025-05-29 20:44:52 -0700},
	note = {https://arxiv.org/abs/2505.22861},
	title = {{Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel}},
	year = {2025}}

@inproceedings{yu2025generics,
	abstract = {Generics, general statements about categories, are believed to transmit essentialist beliefs -- the idea that things have a hidden true nature. Research suggests that people essentialize natural (biological and non-living) and social kinds, but not artifacts. Previous studies using small datasets found that generics are often used to describe animate beings in speech to children. Using a larger corpus of children's books and parent speech, we examined a wider range of kinds and generalizing statements (including habituals and universals). Our results show that generics are more likely used for biological kinds than artifacts and that their use increases in parent speech as children age. However, generics weren't more likely used for non-living or social kinds than artifacts. Habituals, at least in speech, were more likely used for social kinds than artifacts. Generalizing statements were more likely used for about non-living natural kinds than artifacts. These findings inform the debate over whether generics transmit essentialist beliefs.},
	author = {Sunny Yu and Alvin Wei Ming Tan and Siying Zhang and Phillip Mao and Riley Carlson and Tobias Gerstenberg and David Rose},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 11:37:00 -0700},
	date-modified = {2025-05-12 11:38:40 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {{Generics revisited: Analyzing generalizations in children's books and caregivers' speech}},
	year = {2025}}

@inproceedings{btesh2025taking,
	abstract = {This study investigates how individuals balance personal and presentational goals - how they want to be perceived by others - in social interactions where those are conflicting. We develop a computational model that construes presentational goals as minimising the divergence between the perceived and desired belief state of their partner. Based on the divergence between how much a person's partner trusts them versus how much they want to be trusted, we predict complex decisionmaking patterns that cannot arise from solely focusing on maximising a partner's utility. In accordance with our model, participants tended to forego signalling good intentions and prioritised their own goals when they perceived their partner to trust them. Participants were also less concerned about how they were perceived and acted more often in their own interest when their partner was unlikely to change their mind. We show that people are sensitive to the specific belief state of others and can dynamically adjust their decision strategy to trade off presentational and material gains.},
	author = {Victor Btesh and David A. Lagnado and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 11:35:31 -0700},
	date-modified = {2025-05-12 11:35:36 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {{Taking others for granted: Balancing personal and presentational goals in action selection}},
	year = {2025}}

@inproceedings{brockbank2025deep,
	abstract = {When first meeting somebody, we're faced with the challenge of 'getting to know them.' Why do some questions seem to enable this better than others? In Experiment 1, participants (N = 185) evaluated a large bank of conversational questions. We found that questions varied along a reliable latent dimension of interpersonal depth ranging from 'small talk' to 'deep' questions. In Experiment 2 (N = 188), participants answered a subset of these questions along with a number of self-report personality scales. Using a language model to estimate how informative participants' free responses were, we find that individualized personality predictions were more accurate when incorporating free responses; furthermore, responses to deeper questions supported more accurate personality inferences than small talk. Taken together, results suggest not only that responses contained the statistical information necessary to make abstract social inferences, but also that people have accurate intuitions about which conversational topics enable learning about and connecting with others.},
	author = {Erik Brockbank and Tobias Gerstenberg and Judith E. Fan and Robert D. Hawkins},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 11:32:09 -0700},
	date-modified = {2025-05-12 11:33:11 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {{How do we get to know someone? Diagnostic questions for inferring personal traits}},
	year = {2025}}

@inproceedings{yang2025learning,
	author = {Justin Yang and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 11:15:14 -0700},
	date-modified = {2025-05-12 11:15:14 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {Learning to plan from observed and counterfactual experiences},
	year = {2025}}

@inproceedings{teo2025leave,
	abstract = {How do people reason about others when planning deceptive actions? How do detectives infer what suspects did based on the traces their actions left behind? In this work, we explore deception in a setting where agents steal other's snacks and try to determine the most likely thief. We propose a computational model that combines inverse planning with recursive theory of mind to select misleading actions and reason over evidence arising from such plans. In Experiment 1, we demonstrate that suspects strategically modify their behavior when acting deceptively, aligning with our model's predictions. Experiment 2 reveals that detectives show increased uncertainty when evaluating potentially deceptive suspects -- a finding consistent with our model, though alternative explanations exist. Our results suggest that people are adept at deceptive action planning, but struggle to reason about such plans, pointing to possible limits in recursive theory of mind.},
	author = {Verona Teo and Sarah A. Wu and Erik Brockbank and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 11:07:09 -0700},
	date-modified = {2025-05-29 20:47:46 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {{Leave a trace: Recursive reasoning about deceptive behavior}},
	year = {2025}}

@inproceedings{xiang2025how,
	abstract = {How do language models assign responsibility and reward, and is it similar to how humans do it? We instructed three state-of-the-art large language models to assign responsibility (Experiment 1) and reward (Experiment 2) to agents in a collaborative task. We then compared the language models' responses to seven existing cognitive models of responsibility and reward allocation. We found that language models mostly evaluated agents based on force (how much they actually did), in line with classical production-style accounts of causation. By contrast, humans valued actual and counterfactual effort (how much agents tried or could have tried). These results indicate a potential barrier to effective human-machine collaboration.},
	author = {Yang Xiang and Eric Bigelow and Tobias Gerstenberg and Tomer D. Ullman and Samuel Gershman},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 11:04:48 -0700},
	date-modified = {2025-05-12 11:04:48 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {Language models assign responsibility based on actual rather than counterfactual contributions},
	year = {2025}}

@inproceedings{harding2025toward,
	abstract = {This paper presents a formal account of causal explanation, grounded in a theory of conversational pragmatics, and inspired by the interventionist idea that explanation is about asking and answering what-if-things-had-been-different questions. We illustrate the fruitfulness of the account, relative to previous accounts, by showing that widely recognized 'explanatory virtues' emerge naturally, as do subtle empirical patterns concerning the impact of norms on causal judgments. An extended version of the paper with further details can be found here: https://arxiv.org/pdf/2505.03732.},
	author = {Jacqueline Harding and Tobias Gerstenberg and Thomas Icard},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 10:59:43 -0700},
	date-modified = {2025-05-12 11:02:34 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {Toward a formal pragmatics of explanation},
	year = {2025}}

@inproceedings{rose2025fault,
	abstract = {Responsibility requires causation. But there are different kinds of causes. Some are connected to their effects; others are disconnected. We ask how children's developing ability to distinguish causes relates to their understanding of moral responsibility. We found in Experiment 1 that when Andy hits Suzy with his bike, she falls into a fence and it breaks, 3-year-old children treated 'caused', 'break' and 'fault' as referring to the direct cause, Suzy. By 4, they differentiated causes: Andy 'caused' the fence to break, it's his 'fault', but Suzy 'broke' it. We found in Experiment 2 that when the chain involved disconnection, 3-year-olds focused only on the direct cause. Around 5 they distinguished causes, saying that the disconnecting cause 'caused' an object to break, it's their 'fault', but the direct cause 'broke' it. Our findings relate to the outcome-to-intention shift in moral responsibility and suggest a more fundamental shift in children's understanding of causation.},
	author = {David Rose and Cici Hou and Shaun Nichols and Tobias Gerstenberg and Ellen M. Markman},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 09:38:58 -0700},
	date-modified = {2025-05-12 09:44:00 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {Cause and fault in development},
	year = {2025}}

@inproceedings{petrova2025regret,
	abstract = {Regret is a common emotion that might either catalyze or impair decision-making. What determines whether regret will be helpful or harmful in a given situation? We test the hypothesis that regret is more likely to hinder decision-making during the early stages of learning, when information is limited, but help during later stages of learning, when the learner has a better understanding of the environment. We introduce a Bayesian model of learning from regret, in which the 'counterfactual weight' parameter -- reflecting how strongly individuals update their beliefs about foregone outcomes -- predicts both learning outcomes and the intensity of subjective regret. We find that probing regret early in the learning phase leads to worse performance than probing regret later or not at all. This work has important implications for both cognitive and affective science, shedding light on the appraisal mechanisms by which regret influences decision-making.},
	author = {Kate Petrova and James Gross and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 47th Annual Conference of the Cognitive Science Society}},
	date-added = {2025-05-12 09:19:08 -0700},
	date-modified = {2025-05-12 09:21:10 -0700},
	editor = {David Barner and Caren Walker and Azzurra Ruggeri and Neil Bramley},
	title = {{Learning from `what might have been': A Bayesian model of learning from regret}},
	year = {2025}}

@article{harding2025communication,
	abstract = {This paper develops a formal account of causal explanation, grounded in a theory of conversational pragmatics, and inspired by the interventionist idea that explanation is about asking and answering what-if-things-had-been-different questions. We illustrate the fruitfulness of the account, relative to previous accounts, by showing that widely recognized "explanatory virtues" emerge naturally, as do subtle empirical patterns concerning the impact of norms on causal judgments. This shows the value of a "communication-first" approach to explanation: getting clear on explanation's communicative dimension is an important prerequisite for philosophical work on explanation. The result is a simple but powerful framework for incorporating insights from the cognitive sciences into philosophical work on explanation, which will be useful for philosophers or cognitive scientists interested in explanation.},
	author = {Jacqueline Harding and Tobias Gerstenberg and Thomas Icard},
	date-added = {2025-05-07 09:49:44 -0700},
	date-modified = {2025-05-07 09:50:38 -0700},
	journal = {arXiv},
	title = {{A Communication-First Account of Explanation}},
	url = {https://arxiv.org/abs/2505.03732},
	year = {2025},
	bdsk-url-1 = {https://arxiv.org/abs/2505.03732}}

@article{kirfel2025when,
	abstract = {This paper examines the transformative potential of AI embedded with Counterfactual World Simulation Models (CWSMs). A CWSM uses multi-modal evidence, such as the CCTV footage of a road accident, to build a high-fidelity 3D reconstruction of what happened. It can answer causal questions, such as whether the accident happened because the driver was speeding, by simulating what would have happened in relevant counterfactual situations. We sketch a normative and ethical framework that guides and constrains the simulation of counterfactuals. We address the challenge of ensuring fidelity in reconstructions while simultaneously preventing stereotype perpetuation during counterfactual simulations. We anticipate different modes of how users will interact with AI-powered CWSMs and discuss how their outputs may be presented. Finally, we address the prospective applications of CWSMs in the legal domain, recognizing both their potential to revolutionize legal proceedings as well as the ethical concerns they engender. Sketching a new genre of AI, this paper seeks to illuminate the path forward for responsible and effective use of CWSMs.},
	author = {Lara Kirfel and Robert J. MacCoun and Thomas Icard and Tobias Gerstenberg},
	date-added = {2025-04-01 09:22:53 -0700},
	date-modified = {2025-04-01 09:22:53 -0700},
	journal = {AI and Ethics},
	title = {{When AI meets counterfactuals: The ethical implications of counterfactual world simulation models}},
	year = {{in press}}}

@article{rose2025cause,
	abstract = {Although collision-like causes are fundamental in philosophical and psychological theories of causation, humans conceptualize many events as causes that lack direct contact. We argue that how people think and talk about different causes is deeply connected and investigate how children learn this mapping. If Andy hits Suzy with his bike, Suzy falls into a fence and it breaks, Andy caused the fence to break but Suzy broke it. If Suzy forgets sunscreen and gets sunburned, the absence of sunscreen caused Suzy's sunburn, but the sun burned her skin. We tested 691 children and 150 adults. Four-year-old children mapped 'caused' to distal causes and 'broke' to proximal causes (Experiment 1). Though four-year old children didn't map 'caused' to absences until later (Experiment 2), they already referred to absences when asked 'why' an outcome occurred (Experiment 3). Our findings highlight the role of semantics and pragmatics in developing these mappings.},
	author = {Rose, David and Zhang, Siying and Nichols, Shaun and Markman, Ellen and Gerstenberg, Tobias},
	date-added = {2025-02-14 11:15:15 -0800},
	date-modified = {2025-07-14 20:28:38 -0700},
	journal = {PsyArXiv},
	title = {Cause and burn in development},
	url = {https://osf.io/aw834_v1},
	year = {2025},
	bdsk-url-1 = {https://osf.io/aw834_v1}}

@article{davis2025inference,
	abstract = {People have a remarkable ability to infer the hidden causes of things. From physical evidence, such as muddy foot prints on the floor, we can figure out what happened and who did it. Here, we investigate another source of evidence: social evaluations. Social evaluations, such as praise or blame, are commonplace in everyday conversations. While such evaluations don't fully reveal what happened, they provide valuable clues. Across three experiments, we present situations where a person was praised or blamed, and participants' task is to use that information to figure out what happened. In Experiment 1, we find that people draw systematic inferences from social evaluations about situational factors, a person's actions, capabilities, and social roles. In Experiments 2 and 3 we develop computational models that generate praise and blame judgments by considering what causal role a person's action played, and what action they should have taken. Inverting these generative models of praise and blame via Bayesian inference yields accurate predictions about what inferences participants draw based on social evaluations.},
	author = {Davis, Zachary J and Allen, Kelsey R and Kleiman-Weiner, Max and Jara-Ettinger, Julian and Gerstenberg, Tobias},
	date-added = {2025-01-19 13:13:09 -0800},
	date-modified = {2025-01-19 13:13:26 -0800},
	journal = {Journal of Personality and Social Psychology},
	title = {Inference from social evaluation},
	year = {2025}}

@article{xiang2024handicapping,
	abstract = {People use various strategies to bolster the perception of their competence. One strategy is self-handicapping, by which people deliberately impede their performance in order to protect or enhance perceived competence. Despite much prior research, it is unclear why, when, and how self-handicapping occurs. We develop a formal theory that chooses the optimal degree of selfhandicapping based on its anticipated performance and signaling effects. We test the theory's predictions in two experiments (ùëÅ = 400), showing that self-handicapping occurs more often when it is unlikely to affect the outcome and when it increases the perceived competence in the eyes of a naive observer. With sophisticated observers (who consider whether a person chooses to self-handicap), self-handicapping is less effective when followed by failure. We show that the theory also explains the findings of several past studies. By offering a systematic explanation of self-handicapping, the theory lays the groundwork for developing effective interventions.},
	author = {Xiang, Yang and Gershman, Samuel J and Gerstenberg, Tobias},
	date-added = {2024-11-24 10:45:12 -0800},
	date-modified = {2024-11-24 10:45:12 -0800},
	journal = {PsyArXiv},
	note = {https://osf.io/preprints/psyarxiv/84tvm},
	title = {A signaling theory of self-handicapping},
	year = {2024}}

@article{johnson2024wise,
	abstract = {Recent advances in artificial intelligence (AI) have produced systems capable of increasingly sophisticated performance on cognitive tasks. However, AI systems still struggle in critical ways: unpredictable and novel environments (robustness), lack transparency in their reasoning (explainability), face challenges in communication and commitment (cooperation), and pose risks due to potential harmful actions (safety). We argue that these shortcomings stem from one overarching failure: AI systems lack wisdom. Drawing from cognitive and social sciences, we define wisdom as the ability to navigate intractable problems---those that are ambiguous, radically uncertain, novel, chaotic, or computationally explosive---through effective task-level and metacognitive strategies. While AI research has focused on task-level strategies, metacognition---the ability to reflect on and regulate one's thought processes---is underdeveloped in AI systems. In humans, metacognitive strategies such as recognizing the limits of one's knowledge, considering diverse perspectives, and adapting to context are essential for wise decision-making. We propose that integrating metacognitive capabilities into AI systems is crucial for enhancing their robustness, explainability, cooperation, and safety. By focusing on developing wise AI, we suggest an alternative to aligning AI with specific human values---a task fraught with conceptual and practical difficulties. Instead, wise AI systems can thoughtfully navigate complex situations, account for diverse human values, and avoid harmful actions. We discuss potential approaches to building wise AI, including benchmarking metacognitive abilities and training AI systems to employ wise reasoning. Prioritizing metacognition in AI research will lead to systems that act not only intelligently but also wisely in complex, real-world situations.},
	author = {Johnson, Samuel G B and Karimi, Amir-Hossein and Bengio, Yoshua and Chater, Nick and Gerstenberg, Tobias and Larson, Kate and Levine, Sydney and Mitchell, Melanie and Sch{\"o}lkopf, Bernhard and Grossmann, Igor},
	date-added = {2024-11-06 11:16:21 -0600},
	date-modified = {2024-11-06 11:16:21 -0600},
	journal = {arXiv},
	title = {{Imagining and building wise machines: The centrality of AI metacognition}},
	year = {2024}}

@article{jin2024marple,
	abstract = {Reconstructing past events requires reasoning across long time horizons. To figure out what happened, we need to use our prior knowledge about the world and human behavior and draw inferences from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic ``whodunit'' stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze what factors influence inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models. Project website: https: //marple-benchmark.github.io/.},
	annote = {Comment: NeurIPS 2024. First two authors contributed equally. Project page: https://marple-benchmark.github.io/},
	author = {Jin, Emily and Huang, Zhuoyi and Fr{\"a}nken, Jan-Philipp and Liu, Weiyu and Cha, Hannah and Brockbank, Erik and Wu, Sarah and Zhang, Ruohan and Wu, Jiajun and Gerstenberg, Tobias},
	date-added = {2024-10-04 08:51:51 -0700},
	date-modified = {2024-10-04 08:51:51 -0700},
	journal = {arXiv},
	note = {http://arxiv.org/abs/2410.01926},
	title = {{MARPLE: A Benchmark for Long-Horizon Inference}},
	year = {2024}}

@article{gandhi2024affective,
	abstract = {Understanding emotions is fundamental to human interaction and experience. Humans easily infer emotions from situations or facial expressions, situations from emotions, and do a variety of other affective cognition. How adept is modern AI at these inferences? We introduce an evaluation framework for testing affective cognition in foundation models. Starting from psychological theory, we generate 1,280 diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes. We evaluate the abilities of foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully selected conditions. Our results show foundation models tend to agree with human intuitions, matching or exceeding interparticipant agreement. In some conditions, models are ``superhuman'' -- they better predict modal human judgements than the average human. All models benefit from chain-of-thought reasoning. This suggests foundation models have acquired a human-like understanding of emotions and their influence on beliefs and behavior.},
	author = {Kanishk Gandhi and Zoe Lynch and Jan-Philipp Fr{\"a}nken and Kayla Patterson and Sharon Wambu and Tobias Gerstenberg and Desmond C. Ong and Noah D. Goodman},
	date-added = {2024-09-20 10:50:33 -0700},
	date-modified = {2024-09-20 10:50:37 -0700},
	journal = {arXiv},
	note = {https://arxiv.org/abs/2409.11733},
	title = {Human-like Affective Cognition in Foundation Models},
	year = {2024}}

@article{du2024robotic,
	abstract = {When faced with a novel scenario, it can be hard to succeed on the first attempt. In these challenging situations, it is important to know how to retry quickly and meaningfully. Retrying behavior can emerge naturally in robots trained on diverse data, but such robot policies will typically only exhibit undirected retrying behavior and may not terminate a suboptimal approach before an unrecoverable mistake. We can improve these robot policies by instilling an explicit ability to try, evaluate, and retry a diverse range of strategies. We introduce Bellman-Guided Retrials, an algorithm that works on top of a base robot policy by monitoring the robot's progress, detecting when a change of plan is needed, and adapting the executed strategy until the robot succeeds. We start with a base policy trained on expert demonstrations of a variety of scenarios. Then, using the same expert demonstrations, we train a value function to estimate task completion. During test time, we use the value function to compare our expected rate of progress to our achieved rate of progress. If our current strategy fails to make progress at a reasonable rate, we recover the robot and sample a new strategy from the base policy while skewing it away from behaviors that have recently failed. We evaluate our method on simulated and real-world environments that contain a diverse suite of scenarios. We find that Bellman-Guided Retrials increases the average absolute success rates of base policies by more than 20% in simulation and 50% in real-world experiments, demonstrating a promising framework for instilling existing trained policies with explicit trial and error capabilities. For evaluation videos and other documentation, go to https://sites.google.com/view/to-err-robotic/home},
	author = {Du, Maximilian and Khazatsky, Alexander and Gerstenberg, Tobias and Finn, Chelsea},
	date-added = {2024-06-26 12:24:14 -0700},
	date-modified = {2024-06-26 12:25:06 -0700},
	journal = {arXiv},
	title = {{To Err is Robotic: Rapid Value-Based Trial-and-Error during Deployment}},
	url = {http://arxiv.org/abs/2406.15917},
	year = {2024},
	bdsk-url-1 = {http://arxiv.org/abs/2406.15917}}

@inproceedings{wu2024resource,
	abstract = {There is wide agreement that the mind has different mechanisms it can use to make moral judgments. But how does it decide which one to use when? Recent theoretical work has suggested that people select mechanisms of moral judgment in a way that is resource-rational --- that is, by rationally trading off effort against utility. For instance, people may follow general rules in low-stakes situations, but engage more computationally intensive mechanisms such as consequentialist or contractualist reasoning when the stakes are high. Here, we evaluate whether humans and large language models (LLMs) exhibit resource-rational moral reasoning in two moral dilemmas by manipulating the stakes of each scenario. As predicted, we found that the higher the stakes, the more people employed a more effortful mechanism over following a general rule. However, there was mixed evidence for similar resource-rational moral reasoning in the LLMs. Our results provide evidence that people's moral judgments reflect resource-rational cognitive constraints, and they highlight the opportunities for developing AI systems better aligned with human moral values.},
	author = {Sarah A. Wu and Xiang Ren and Tobias Gerstenberg and Yejin Choi and Sydney Levine},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-06-07 16:49:16 -0700},
	date-modified = {2024-06-07 16:50:55 -0700},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {Resource-rational moral judgment},
	year = {2024}}

@inproceedings{wu2024whodunnit,
	abstract = {Humans are remarkably adept at inferring the causes of events in their environment; doing so often requires incorporating information from multiple sensory modalities. For instance, if a car slows down in front of us, inferences about why they did so are rapidly revised if we also hear sirens in the distance. Here, we investigate the ability to reconstruct others' actions and events from the past by integrating multimodal information. Participants were asked to infer which of two agents performed an action in a household setting given either visual evidence, auditory evidence, or both. We develop a computational model that makes inferences by generating multimodal simulations, and also evaluate our task on a large language model (GPT-4) and a large multimodal model (GPT-4V). We find that humans are relatively accurate overall and perform best when given multimodal evidence. GPT-4 and GPT-4V performance comes close overall, but is very weakly correlated with participants across individual trials. Meanwhile, the simulation model captures the pattern of human responses well. Multimodal event reconstruction represents a challenge for current AI systems, and frameworks that draw on the cognitive processes underlying people's ability to reconstruct events offer a promising avenue forward.},
	author = {Wu, Sarah A and Brockbank, Erik and Cha, Hannah and Fr{\"a}nken, Jan-Philipp and Jin, Emily and Huang, Zhuoyi and Liu, Weiyu and Zhang, Ruohan and Wu, Jiajun and Gerstenberg, Tobias},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-05-13 09:25:14 +0200},
	date-modified = {2024-05-13 09:25:14 +0200},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {{Whodunnit? Inferring what happened from multimodal evidence}},
	year = {2024}}

@inproceedings{tsirtsis2024sequential,
	abstract = {When a human and an AI agent collaborate to complete a task and something goes wrong, who is responsible? Prior work has developed theories to describe how people assign responsibility to individuals in teams. However, there has been little work studying the cognitive processes that underlie responsibility judgments in human-AI collaborations, especially for tasks comprising a sequence of interdependent actions. In this work, we take a step towards filling this gap. Using semiautonomous driving as a paradigm, we develop an environment that simulates stylized cases of human-AI collaboration using a generative model of agent behavior. We propose a model of responsibility that considers how unexpected an agent's action was, and what would have happened had they acted differently. We test the model's predictions empirically and find that in addition to action expectations and counterfactual considerations, participants' responsibility judgments are also affected by how much each agent actually contributed to the outcome.},
	author = {Tsirtsis, Stratis and Gomez-Rodriguez, Manuel and Gerstenberg, Tobias},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-05-12 17:48:51 +0200},
	date-modified = {2024-05-12 17:48:51 +0200},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {{Towards a computational model of responsibility judgments in sequential human-AI collaboration}},
	year = {2024}}

@inproceedings{keshmirian2024chain,
	abstract = {Causal reasoning is important for humans and artificial intelligence (AI). Causal Bayesian Networks (CBNs) model causal relationships using directed links between nodes in a network. Deviations from their edicts result in biased judgments. This study explores one such bias by examining two structures in CBNs: canonical Chain (A‚ÜíC‚ÜíB) and Common Cause (A‚ÜêC‚ÜíB) networks. In these structures, if C is known, the probability of the outcome (B) is normatively independent of the initial cause (A). But humans often ignore this independence. We tested mutually exclusive predictions of three theories that could account for this bias (N=300). Our results show that humans perceive causes in Chain structures as significantly stronger, supporting only one of the hypotheses. The increased perceived causal power might reflect a view of intermediate causes as more reflective of reliable mechanisms. The bias may stem either from our interventions in the world or how we talk about causality with others. LLMs are primarily trained on language data. Therefore, examining whether they exhibit similar biases can determine the extent to which language is the vehicle of such causal biases, with implications for whether LLMs can abstract causal principles. We therefore, subjected three LLMs, GPT3.5-Turbo, GPT4, and Luminous Supreme Control, to the same queries as our human subjects, adjusting a key `temperature' hyperparameter. We show that at greater randomness levels, LLMs exhibit a similar bias, suggesting it is supported by language use. The absence of item effects suggests a degree of causal principle abstraction in LLMs.},
	author = {Keshmirian, Anita and Willig, Moritz and Hemmatian, Babak and Hahn, Ulrike and Kersting, Kristian and Gerstenberg, Tobias},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-05-12 17:47:58 +0200},
	date-modified = {2024-06-07 21:28:31 -0700},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {{Chain versus common cause: Biased causal strength judgments in humans and large language models}},
	year = {2024}}

@inproceedings{brockbank2024monster,
	abstract = {Everyday reasoning about others involves accounting for why they act the way they do. With many explanations for someone's behavior, how do observers choose the best one? A large body of work in social psychology suggests that people's explanations rely heavily on traits rather than external factors. Recent results have called this into question, arguing that people balance traits, mental states, and situation to make sense of others' actions. How might they achieve this? In the current work, we hypothesize that people rely on counterfactual simulation to weigh different explanations for others' behavior. We propose a computational model of this process that makes concrete predictions about when people will prefer to explain events based on the actor's traits or their situation. We test the predictions of this model in an experimental paradigm in which trait and situation each guide behavior to varying degrees. Our model predicts people's causal judgments well overall but is less accurate for trait explanations than situational explanations. In a comparison with simpler causal heuristics, a majority of participants were better predicted by the counterfactual model. These results point the way toward a more comprehensive understanding of how social reasoning is performed within the context of domain-general causal inference.},
	author = {Brockbank, Erik and and Yang, Justin and Govil, Mishika and Fan, Judith E and Gerstenberg, Tobias},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-05-12 17:46:35 +0200},
	date-modified = {2024-05-14 10:59:35 +0200},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {{Without his cookies, he's just a monster: A counterfactual simulation model of social explanation}},
	year = {2024}}

@inproceedings{kirfel2024do,
	abstract = {People often select only a few events when explaining what happened. What drives people's explanation selection? Prior research argued that people's explanation choices are affected by event normality and causal structure. Here, we propose a new model of these existing findings and test its predictions in a novel experiment. The model predicts that speakers value accuracy and relevance. They choose explanations that are true, and that communicate useful information to the listener. We test the model's predictions empirically by manipulating what goals a listener has and what actions they can take. Across twelve experimental conditions, we find that our model accurately predicts that people like to choose explanations that communicate optimal interventions.},
	author = {Kirfel, Lara and Harding, Jacqueline and Shin, Jeong and Xin, Cindy and Icard, Thomas and Gerstenberg, Tobias},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-05-09 09:05:13 -0700},
	date-modified = {2024-05-09 09:10:57 -0700},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {{Do as I explain: Explanations communicate optimal interventions}},
	year = {2024}}

@inproceedings{keshmirian2024biased,
	abstract = {Causal reasoning is a critical aspect of both human cognition and artificial intelligence (AI), playing a prominent role in understanding the relationships between events. Causal Bayesian Networks (CBNs) have been instrumental in modeling such relationships, using directed, acyclic links between nodes in a network to depict probabilistic associations between variables. Deviations from these graphical models' edicts would result in biased judgments. This study explores one such bias in the causal judgments of humans and Large Language Models (LLMs) by examining two structures in CBNs: Canonical Chain (A‚ÜíB‚ÜíC) and Common Cause (A‚ÜêB‚ÜíC) networks. In these structures, once the intermediate variable (B) is known, the probability of the outcome (C) is normatively independent of the initial cause (A). However, studies have shown that humans often ignore this independence. We tested the mutually exclusive predictions of three theories that could account for this bias (N=300). Using hierarchical mixed-effect models, we found that humans tend to perceive causes in Chain structures as significantly stronger, providing support for only one of the hypotheses. This increase in perceived causal power might reflect a view of intermediate causes as more reflective of reliable mechanisms, which could, in turn, stem from our interactions with the world or the way we communicate causality to others. LLMs are primarily trained on language data. Therefore, examining whether they exhibit similar biases in causal reasoning can help us understand the origins of canonical Chain structures' perceived causal power while also shedding light on whether LLMs can abstract causal principles. To investigate this, we subjected three LLMs, GPT3.5-Turbo, GPT4, and Luminous Supreme Control, to the same queries as our human subjects, adjusting a key `temperature' hyperparameter. Our findings show that, particularly with higher temperatures (i.e., greater randomness), LLMs exhibit a similar boost in the perceived causal power of Chains, suggesting the bias is at least partly reflected in language use. Similar results across items suggest a degree of causal principle abstraction in the studied models. Implications for causal representation in humans and LLMs are discussed.},
	author = {Anita Keshmirian and Moritz Willig and Babak Hemmatian and Ulrike Hahn and Kristian Kersting and Tobias Gerstenberg},
	booktitle = {ICLR 2024 Workshop on Representational Alignment},
	date-added = {2024-05-03 18:39:05 -0400},
	date-modified = {2024-05-03 18:39:34 -0400},
	title = {Biased Causal Strength Judgments in Humans and Large Language Models},
	url = {https://openreview.net/forum?id=544P6YidFk},
	year = {2024},
	bdsk-url-1 = {https://openreview.net/forum?id=544P6YidFk}}

@article{franken2024sami,
	abstract = {When prompting a language model (LM), users frequently expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles into a model can be resource-intensive and technically challenging, generally requiring human preference labels or examples. We introduce SAMI, a method for teaching a pretrained LM to follow behavioral principles that does not require any preference labels or demonstrations. SAMI is an iterative algorithm that finetunes a pretrained LM to increase the conditional mutual information between constitutions and self-generated responses given queries from a datasest. On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%. Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue. SAMI requires a "principle writer" model; to avoid dependence on stronger models, we further evaluate aligning a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct). The SAMI-trained mixtral-8x7b outperforms both the initial model and the instruction-finetuned model, achieving a 65% win rate on summarization. Our results indicate that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight.},
	author = {Fr{\"a}nken, Jan-Philipp and Zelikman, Eric and Rafailov, Rafael and Gandhi, Kanishk and Gerstenberg, Tobias and Goodman, Noah D.},
	date-added = {2024-04-22 21:44:23 -0700},
	date-modified = {2024-04-22 21:44:23 -0700},
	journal = {arXiv},
	title = {{Self-supervised alignment with mutual information: Learning to follow principles without preference labels}},
	url = {http://arxiv.org/abs/2404.14313},
	year = {2024},
	bdsk-url-1 = {http://arxiv.org/abs/2404.14313}}

@inproceedings{franken2024rails,
	abstract = {As AI systems like language models are increasingly integrated into decision-making processes affecting people's lives, it's critical to ensure that these systems have sound moral reasoning. To test whether they do, we need to develop systematic evaluations. We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates. With this framework, we procedurally generated a large and diverse set of moral dilemmas---the OffTheRails benchmark---consisting of 50 scenarios and 400 unique test items. We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions. We find that moral dilemmas in which the harm is a necessary means (as compared to a side effect) resulted in lower permissibility and higher intention ratings for both participants and language models. The same pattern was observed for evitable versus inevitable harmful outcomes. However, there was no clear effect of whether the harm resulted from an agent's action versus from having omitted to act. We discuss limitations of our prompt generation pipeline and opportunities for improving scenarios to increase the strength of experimental effects.},
	author = {Jan-Philipp Fr{\"a}nken and Kanishk Gandhi and Tori Qiu and Ayesha Khawaja and Noah D. Goodman and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 46th Annual Conference of the Cognitive Science Society}},
	date-added = {2024-04-17 14:21:36 -0700},
	date-modified = {2024-05-09 09:13:58 -0700},
	editor = {Larissa K Samuelson and Stefan Frank and Mariya Toneva and Allyson Mackey and Eliot Hazeltine},
	title = {Procedural dilemma generation for evaluating moral reasoning in humans and language models},
	year = {2024}}

@article{andukuri2024stargate,
	abstract = {When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model's ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions---a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model---the Questioner---and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer's latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on 72% of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.},
	author = {Andukuri, Chinmaya and Fr{\"a}nken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
	date-added = {2024-03-31 21:10:58 -0700},
	date-modified = {2024-03-31 21:12:11 -0700},
	journal = {arXiv},
	title = {{STaR}-{GATE}: {Teaching} {Language} {Models} to {Ask} {Clarifying} {Questions}},
	url = {http://arxiv.org/abs/2403.19154},
	year = {2024},
	bdsk-url-1 = {http://arxiv.org/abs/2403.19154}}

@inproceedings{vodrahalli2022humans,
	abstract = {In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, 'activation-integration' model for human behavior and use it to characterize the factors that affect human-AI interactions.},
	author = {Vodrahalli, Kailas and Daneshjou, Roxana and Gerstenberg, Tobias and Zou, James},
	booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
	date-added = {2024-02-26 20:00:46 -0800},
	date-modified = {2024-02-26 20:02:18 -0800},
	pages = {763--777},
	title = {{Do humans trust advice more if it comes from AI? an analysis of human-AI interactions}},
	year = {2022}}

@article{vodrahalli2022uncalibrated,
	abstract = {In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of 'confidence' that the human can use to calibrate how much they depend on or trust the advice. In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks---dealing with images, text and tabular data---involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone.},
	author = {Vodrahalli, Kailas and Gerstenberg, Tobias and Zou, James Y},
	date-added = {2024-02-26 20:00:46 -0800},
	date-modified = {2024-02-26 20:03:41 -0800},
	journal = {Advances in Neural Information Processing Systems},
	pages = {4004--4016},
	title = {{Uncalibrated models can improve human-AI collaboration}},
	volume = {35},
	year = {2022}}

@article{prinzing2025purpose,
	abstract = {People attribute purposes in both mundane and profound ways---such as when thinking about the purpose of a knife and the purpose of a life. In three studies (total N = 13,720 observations from N = 3,430 participants), we tested whether these seemingly very different forms of purpose attributions might actually involve the same cognitive processes. We examined the impacts of four factors on purpose attributions in six domains (artifacts, social institutions, animals, body parts, sacred objects, and human lives). Study 1 manipulated what items in each domain were originally created for (original design) and how people currently use them (present practice). Study 2 manipulated whether items are good at achieving a goal (effectiveness) and whether the goal itself is good (morality). We found effects of each factor in every domain. However, whereas morality and effectiveness had remarkably similar effects across domains, the effects of original design and present practice differed substantially. Finally, Study 3 revealed that, within domains, the effects of original design and present practice depend on which entities design and use items. These results reveal striking similarities in purpose attributions across domains and suggest that certain entities are treated as authorities over the purposes of particular items.},
	author = {Michael Prinzing and David Rose and Siying Zhang and Eric Tu and Abigail Concha and Michael Rea and Jonathan Schaffer and Tobias Gerstenberg and Joshua Knobe},
	date-added = {2024-01-25 13:55:21 -0800},
	date-modified = {2025-01-19 13:15:53 -0800},
	journal = {Journal of Experimental Psychology: General},
	note = {https://osf.io/7enkr},
	title = {{From artifacts to human lives: Investigating the domain-generality of judgments about purposes}},
	year = {2025},
	bdsk-url-1 = {https://osf.io/7enkr}}

@article{gerstenberg2024counterfactual,
	abstract = {How do people make causal judgments and assign responsibility? In this paper, I show that counterfactual simulations are key. To simulate counterfactuals, we need three ingredients: a generative mental model of the world, the ability to perform counterfactual interventions on that model, and the capacity to simulate the consequences of these interventions. The counterfactual simulation model (CSM) uses these ingredients to capture people's intuitive understanding of the physical and social world. In the physical domain, the CSM predicts people's causal judgments about dynamic collision events, complex situations that involve multiple causes, omissions as causes, and causes that sustain physical stability. In the social domain, the CSM predicts responsibility judgments in helping and hindering scenarios.},
	author = {Tobias Gerstenberg},
	date-added = {2024-01-21 16:57:04 -0800},
	date-modified = {2024-05-15 11:10:14 +0200},
	journal = {Trends in Cognitive Sciences},
	title = {Counterfactual simulation in causal cognition},
	url = {https://osf.io/preprints/psyarxiv/72scr},
	year = {2024},
	bdsk-url-1 = {https://osf.io/preprints/psyarxiv/72scr}}

@inproceedings{nie2023moca,
	abstract = {Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.},
	author = {Allen Nie and Yuhui Zhang and Atharva Amdekar and Christopher J Piech and Tatsunori Hashimoto and Tobias Gerstenberg},
	booktitle = {{Advances in Neural Information Processing Systems}},
	date-added = {2023-10-30 18:43:34 -0700},
	date-modified = {2023-10-30 18:45:53 -0700},
	title = {{MoCa: Measuring human-language model alignment on causal and moral judgment tasks}},
	year = {2023}}

@inproceedings{kirfel2023anticipating,
	abstract = {This paper examines the transformative potential of Counterfactual World Simulation Models (CWSMs). CWSMs use pieces of multi-modal evidence, such as the CCTV footage or sound recordings of a road accident, to build a high-fidelity 3D reconstruction of the scene. They can also answer causal questions, such as whether the accident happened because the driver was speeding, by simulating what would have happened in relevant counterfactual situations. CWSMs will enhance our capacity to envision alternate realities and investigate the outcomes of counterfactual alterations to how events unfold. This also, however, raises questions about what alternative scenarios we should be considering and what to do with that knowledge. We present a normative and ethical framework that guides and constrains the simulation of counterfactuals. We address the challenge of ensuring fidelity in reconstructions while simultaneously preventing stereotype perpetuation during counterfactual simulations. We anticipate different modes of how users will interact with CWSMs and discuss how their outputs may be presented. Finally, we address the prospective applications of CWSMs in the legal domain, recognizing both their potential to revolutionize legal proceedings as well as the ethical concerns they engender. Anticipating a new type of AI, this paper seeks to illuminate a path forward for responsible and effective use of CWSMs.},
	author = {Lara Kirfel and Robert J. MacCoun and Thomas Icard and Tobias Gerstenberg},
	booktitle = {{AI Meets Moral Philosophy and Moral Psychology Workshop (NeurIPS 2023)}},
	date-added = {2023-10-30 18:20:41 -0700},
	date-modified = {2023-11-20 10:18:20 -0800},
	title = {Anticipating the risks and benefits of counterfactual world simulation models},
	year = {2023}}

@inproceedings{franken2023rails,
	abstract = {As AI systems like language models are increasingly integrated into making decisions that affect people, it's critical to ensure that these systems have sound moral reasoning. To test whether they do, we need to develop systematic evaluations. Recent work has introduced a method for procedurally generating LLM evaluations from abstract causal templates, and tested this method in the context of social reasoning (i.e., theory-of-mind). In this paper, we extend this method to the domain of moral dilemmas. We develop a framework that translates causal graphs into a prompt template which can then be used to procedurally generate a large and diverse set of moral dilemmas using a language model. Using this framework, we created the OffTheRails dataset which consists of 50 scenarios and 500 unique test items. We evaluated the quality of our model-written test items using two independent human experts and found that 90% of the test-items met the desired structure. We collect moral permissibility and intention judgments from 100 human crowdworkers and compared these judgments with those from GPT-4 and Claude-2 across eight control conditions. Both humans and GPT-4 assigned higher intentionality to agents when a harmful outcome was evitable and a necessary means. However, our findings did not match previous findings on permissibility judgments. This difference may be a result of not controlling the severity of harmful outcomes during scenario generation. We conclude by discussing future extensions of our benchmark to address this limitation.},
	author = {Jan-Philipp Fr{\"a}nken and Ayesha Khawaja and Kanishk Gandhi and Jared Moore and Noah D. Goodman and Tobias Gerstenberg},
	booktitle = {{AI Meets Moral Philosophy and Moral Psychology Workshop (NeurIPS 2023)}},
	date-added = {2023-10-30 18:18:28 -0700},
	date-modified = {2023-10-30 18:18:35 -0700},
	title = {Off The Rails: Procedural Dilemma Generation for Moral Reasoning},
	year = {2023}}

@inproceedings{franken2023social,
	abstract = {We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation frameworks in which AI assistants need to infer preferences from diverse users can provide a valuable approach for studying practical alignment questions.},
	author = {Jan-Philipp Fr{\"a}nken and Sam Kwok and Peixuan Ye and Kanishk Gandhi and Dilip Arumugam and Jared Moore and Alex Tamkin and Tobias Gerstenberg and Noah D. Goodman},
	booktitle = {Socially Responsible Language Modelling Research Workshop (NeurIPS 2023)},
	date-added = {2023-10-30 18:11:09 -0700},
	date-modified = {2023-10-30 18:11:15 -0700},
	title = {Social Contract AI: Aligning AI Assistants with Implicit Group Norms},
	year = {2023}}

@incollection{goodman2024probabilistic,
	author = {Noah D. Goodman and Tobias Gerstenberg and Joshua B. Tenenbaum},
	booktitle = {Reverse-engineering the mind: The Bayesian approach to cognitive science},
	editor = {Thomas L. Griffiths and Nick Chater and Joshua B. Tenenbaum},
	publisher = {Mit Press},
	title = {Probabilistic programs as a unifying language of thought},
	year = {2024}}

@article{amemiya2024disagreement,
	abstract = {In a rapidly changing and diverse world, the ability to reason about conflicting perspectives is critical for effective communication, collaboration, and critical thinking. The current pre-registered experiments with children ages 7 to 11 years investigated the developmental foundations of this ability through a novel social reasoning paradigm and a computational approach. In the inference task, children were asked to figure out what happened based on whether two speakers agreed or disagreed in their interpretation. In the prediction task, children were provided information about what happened and asked to predict whether two speakers will agree or disagree. Together, these experiments assessed children's understanding that disagreement often results from ambiguity about what happened, and that ambiguity about what happened is often predictive of disagreement. Experiment 1 (N =52) showed that children are more likely to infer that an ambiguous utterance occurred after learning that people disagreed (versus agreed) about what happened and found that these inferences become stronger with age. Experiment 2 (N =110) similarly found age-related change in children's inferences and also showed that children could reason in the forward direction, predicting that an ambiguous utterance would lead to disagreement. A computational model indicated that although children's ability to predict when disagreements might arise may be critical for making the reverse inferences, it did not fully account for age-related change.},
	author = {Jamie Amemiya and Gail D. Heyman and Tobias Gerstenberg},
	date-added = {2023-09-09 11:57:43 -0700},
	date-modified = {2024-06-05 08:13:16 -0700},
	journal = {Cognition},
	title = {Children use disagreement to infer what happened},
	url = {https://psyarxiv.com/y79sd/},
	year = {2024},
	bdsk-url-1 = {https://psyarxiv.com/y79sd/}}

@article{beller2025causation,
	abstract = {The words we use to describe what happened shape what comes to a listener's mind. How do speakers choose what causal expressions to use? How does that choice impact what listeners imagine? In this paper, we develop a computational model of how people use the causal expressions "caused", "enabled", "affected", and "made no difference". The model first builds a causal representation of what happened. By running counterfactual simulations, the model computes several causal aspects that capture the different ways in which a candidate cause made a difference to the outcome. Logical combinations of these aspects define a semantics for the causal expressions. The model then uses pragmatic inference to decide what word to use in context. We test our model in a series of experiments and compare it to prior psychological accounts. In a set of psycholinguistic studies, we verify the model's semantics and pragmatics. We show that the causal expressions exist on a hierarchy of specificity, and that participants draw informative pragmatic inferences in line with this scale. In the next two studies, we demonstrate that our model quantitatively fits participant behavior in a speaker task and a listener task involving dynamic physical scenarios. We compare our model to two lesioned alternatives, one which removes pragmatic inference, and another which removes semantics and pragmatics. Our full model better accounts for participants' behavior than both alternatives. Taken together, these results suggest a new way forward for modeling the relationship between language and thought in the study of causality.},
	author = {Aaron Beller and Tobias Gerstenberg},
	date-added = {2023-07-04 14:31:53 -0700},
	date-modified = {2025-02-03 09:32:53 -0800},
	journal = {Psychological Review},
	title = {Causation, Meaning, and Communication},
	url = {https://psyarxiv.com/xv8hf},
	year = {2025},
	bdsk-url-1 = {https://psyarxiv.com/xv8hf}}

@inproceedings{chase2023realism,
	abstract = {Interacting in real environments, such as manipulating objects, involves multisensory information. However, little is known about how multisensory cue characteristics help us determine what has occurred in a scene, including whether two events were causally linked. In virtual environments, the number of sensory modalities present and levels of realism often vary. In this work, we explore what role multisensory information and physical realism play in people's causal perception. So far, haptic cues have rarely been studied in causal perception. Here, we combined visual, auditory, and haptic cues in a psychophysical study in which participants were asked to judge whether one billiard ball caused another to move. We manipulated the temporal delay between cause and effect events, and the physical realism of each cue. While temporal delays generally decreased causal judgments, the number of multisensory cues and their physical realism increased causal judgments. We highlight the implications of this work for building immersive environments.},
	author = {Elyse Chase and Tobias Gerstenberg and Sean Follmer},
	booktitle = {IEEE World Haptics Conference (WHC)},
	date-added = {2023-06-29 09:40:39 -0700},
	date-modified = {2023-06-29 09:45:29 -0700},
	title = {Realism of Visual, Auditory, and Haptic Cues in Phenomenal Causality},
	year = {2023}}

@inproceedings{gandhi2023understanding,
	abstract = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
	author = {Kanishk Gandhi and Jan-Philipp Fr{\"a}nken and Tobias Gerstenberg and Noah D. Goodman},
	booktitle = {{Advances in Neural Information Processing Systems}},
	date-added = {2023-06-27 19:45:01 -0700},
	date-modified = {2023-10-30 18:11:40 -0700},
	title = {Understanding Social Reasoning in Language Models with Language Models},
	year = {2023}}

@article{vasconcelos2023explanations,
	author = {Vasconcelos, Helena and J{\"o}rke, Matthew and Grunde-McLaughlin, Madeleine and Gerstenberg, Tobias and Bernstein, Michael S and Krishna, Ranjay},
	date-added = {2023-05-31 16:57:56 +0200},
	date-modified = {2023-05-31 16:57:56 +0200},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	number = {CSCW1},
	pages = {1--38},
	publisher = {ACM New York, NY, USA},
	title = {Explanations can reduce overreliance on ai systems during decision-making},
	volume = {7},
	year = {2023}}

@inproceedings{cao2023semantics,
	abstract = {When choosing how to describe what happened, we have a number of causal verbs at our disposal. In this paper, we develop a model-theoretic formal semantics for nine causal verbs that span the categories of CAUSE, ENABLE, and PREVENT. We use structural causal models (SCMs) to represent participants' mental construction of a scene when assessing the correctness of causal expressions relative to a presented context. Furthermore, SCMs enable us to model events relating both the physical world as well as agents' mental states. In experimental evaluations, we find that the proposed semantics exhibits a closer alignment with human evaluations in comparison to prior accounts of the verb families.},
	author = {Angela Cao and Atticus Geiger and Elisa Kreiss and Thomas Icard and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-14 15:43:57 -0700},
	date-modified = {2023-05-14 15:46:35 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {A Semantics for Causing, Enabling, and Preventing Verbs Using Structural Causal Models},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@inproceedings{gonzalez2023agents,
	abstract = {This work attempts to bridge the divide between accounts of causal reasoning with respect to agents and objects. We begin by examining the influence of animacy. In a collision-based context, we vary the animacy status of an object using 3D animations. By holding the fine-grained kinematics of the actual and counterfactual outcomes fixed across animate and inanimate conditions, we find that animacy itself has no effect on causal attribution judgments. Next, we test if causal judgments for animate and inanimate objects differ as a function of the counterfactuals they respectively afford in a disjunctive causal structure. Here, we find that the effect of perceived animacy on causal attribution is mediated by differences in counterfactual judgments. Finally, we introduce the known effect of prescriptive norm violations to this paradigm. Our results collectively highlight how normative expectations specify the counterfactual considerations that guide causal reasoning about both agents and objects.},
	author = {Bryan Gonzalez and Tobias Gerstenberg and Jonathan Phillips},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-11 14:25:35 -0700},
	date-modified = {2023-05-11 14:26:01 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {Causal Reasoning Across Agents and Objects},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@inproceedings{nam2023show,
	abstract = {There are at least three ways of learning how the world works: learning from observations, from interventions, and from explanations. Prior work on causal inference focused on how people learn causal structures through observation and intervention. Our study is the first to look at how explanations support causal structure learning. We develop a normative inference model that learns from observations and explanations, and compare the model's predictions to participants' judgments. The task is to infer the causal connections in 3-node graphs, based on information about their co-activation, and explanations of the kind "B activated because A activated". We find that participants learn better from explanations than from observations. However, while the normative model benefits from having observations in addition to explanations, participants did not.},
	author = {Andrew Nam and Christopher Hughes and Thomas Icard and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-11 14:20:51 -0700},
	date-modified = {2023-05-11 14:21:09 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {Show and tell: Learning causal structures from observations and explanations},
	url = {https://psyarxiv.com/wjs9q},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@inproceedings{zhang2023llm,
	abstract = {How do essentialist beliefs about categories arise? We hypothesize that such beliefs are transmitted via language. We subject large language models (LLMs) to vignettes from the literature on essentialist categorization and find that they align well with people when the studies manipulated teleological information - information about what something is for. We examine whether in a classic test of essentialist categorization - the transformation task - LLMs prioritize teleological properties over information about what something looks like, or is made of. Experiments 1 and 2 find that telos and what something is made of matter more than appearance. Experiment 3 manipulates all three factors and finds that what something is for matters more than what it's made of. Overall, these studies suggest that language alone may be sufficient to give rise to essentialist beliefs, and that information about what something is for matters more.},
	author = {Siying Zhang and Jingyuan S. She and Tobias Gerstenberg and David Rose},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-11 14:04:02 -0700},
	date-modified = {2023-05-11 14:04:54 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {You are what you're for: Essentialist categorization in large language models},
	url = {https://psyarxiv.com/ypw5r/},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@inproceedings{kirfel2023willful,
	abstract = {What someone knew matters for how we hold them responsible. In three studies, we explore people's responsibility judgments for negative outcomes to knowledgeable versus ignorant agents. We manipulate whether agents arrived at their knowledge state unintentionally or willfully. In Experiment 1, agents who knew about the harmful consequences of their actions were judged highly responsible no matter how they came to know. In contrast, willfully ignorant agents were judged more responsible than unintentionally ignorant agents. Participants inferred that willfully ignorant agents were more likely to believe that their action might cause harm. When we explicitly stipulate the agents' beliefs in Experiment 2, the 'willful ignorance' effect reduces but persists. Participants inferred that the willfully ignorant agent was more likely to have acted anyhow even if they had known. Explicitly stating whether the agent's action depended on their knowledge further reduced the 'willful ignorance' effect in Experiment 3.},
	author = {Lara Kirfel and Xenia Bunk and Ro'i Zultan and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-11 13:58:15 -0700},
	date-modified = {2023-05-11 13:59:25 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {Father, don't forgive them, for they could have known what they're doing},
	url = {https://psyarxiv.com/a87xm/},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@inproceedings{wu2023computational,
	abstract = {How responsible someone is for an outcome depends on both the causal role of their actions, and what those actions reveal about their moral character. Prior work has successfully modeled people's causal attributions and mental state inferences using planning algorithms assumed to approximate people's intuitive theory of mind about others' behavior. In this paper, we develop a unified computational framework for responsibility judgments in which the same generative planner can model both of these processes. We test our framework on a variety of animated social scenarios in two experiments. Experiment 1 features simple cases of helping and hindering. Experiment 2 features more complex interactions that require recursive reasoning, including cases where one agent affects another by merely signaling their intentions without physically acting on the world. Our model accurately captures participants' counterfactual simulations and intention inferences. Together, these two factors explain responsibility judgments.},
	author = {Sarah A. Wu and Shruti Sridhar and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-11 13:43:09 -0700},
	date-modified = {2023-05-11 14:37:19 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {A computational model of responsibility judgments from counterfactual simulations and intention inferences},
	url = {https://psyarxiv.com/uwdbr/},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@inproceedings{rose2023teleology,
	abstract = {Generic statements, such as "Bees are striped" are thought to be a central vehicle by which essentialist beliefs are transmitted. But work on generics and essentialism almost never focuses on the type of properties mentioned in generic statements. We test the hypothesis that teleological properties, what something is for, affect categorization judgments more strongly than behavioral, biological, or social properties. In Experiment 1, participants categorized properties as being either behavioral, biological, social, or teleological. In Experiment 2, we used the top four properties from each group to describe a generic noun or a specific individual. Participants then categorized creatures that had one of their properties transformed. We found that changes to teleological properties had the strongest impact on categorization judgments. In Experiment 3, we also found that teleological properties mattered more in an induction task. We suggest that teleological properties play this privileged role in categorization because they are treated as essential properties.},
	author = {David Rose and Siying Zhang and Qi Han and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-09 11:26:32 -0700},
	date-modified = {2023-05-09 11:28:02 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {Teleology and generics},
	year = {2023}}

@inproceedings{shin2023abstraction,
	abstract = {What shape do people's mental models take? We hypothesize that people build causal models that are suited to the task at hand. These models abstract away information to represent what matters. To test this idea empirically, we presented participants with causal learning paradigms where some features were outcome-relevant and others weren't. In Experiment 1, participants had to learn what objects of different shape and color made a machine turn on. In Experiment 2, they had to predict whether blocks sliding down ramps would cross a finish line. In both experiments, participants made systematic errors in a surprise test that asked them to recall what they had seen earlier. The errors people made suggest that they had built mental models of the task that privileged causally relevant information. Our results contribute to recent efforts trying to characterize the important role that causal abstraction plays in human learning and inference.},
	author = {Steven M Shin and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-09 09:47:03 -0700},
	date-modified = {2023-05-09 09:47:03 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {Learning what matters: Causal abstraction in human inference},
	url = {https://psyarxiv.com/br2vz},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@article{wu2023replacement,
	abstract = {How do people hold others responsible? Responsibility judgments are affected not only by what actually happened, but also by what could have happened if things had turned out differently. Here, we look at how replaceability -- the ease with which a person could have been replaced by someone else -- affects responsibility. We develop the counterfactual replacement model which runs simulations of alternative scenarios to determine the probability that the outcome would have been different if the person of interest had been replaced. The model predicts that a person is held more responsible when it would have been more difficult to replace them. To test the model's predictions, we design a paradigm that quantitatively varies replaceability by manipulating the number of replacements as well as the probability with which each replacement would have been available. Across three experiments featuring increasingly complex scenarios, we show that the model explains participants' responsibility judgments well in both social and physical settings, and better than alternative models that rely only on features of what actually happened.},
	author = {Sarah A Wu and Tobias Gerstenberg},
	date-added = {2023-03-07 18:12:16 -0800},
	date-modified = {2023-10-02 22:32:06 -0700},
	journal = {Cognition},
	title = {{If not me, then who? Responsibility and replacement}},
	year = {{accepted}},
	bdsk-url-1 = {https://psyarxiv.com/m2rcj/}}

@article{gerstenberg2023criticality,
	abstract = {How critical are individual members perceived to be for their group's performance? In this paper, we show that judgments of criticality are intimately linked to considering responsibility. Prospective responsibility attributions in groups are relevant across many domains and situations, and have the potential to influence motivation, performance, and allocations of resources. We develop various models that differ in how the relationship between criticality and responsibility is conceptualized. To test our models, we experimentally vary the task structure (disjunctive, conjunctive, and mixed) and the abilities of the group members (which affects their probability of success). We show that both factors influence criticality judgments, and that a model which construes criticality as anticipated credit best explains participants' judgments. Unlike prior work that has defined criticality as anticipated responsibility for both success and failures, our results suggest that people only consider the possible outcomes in which an individual contributed to a group success, but disregard group failure.},
	author = {Tobias Gerstenberg and David A. Lagnado and Ro'i Zultan},
	date-added = {2023-02-16 10:59:14 -0800},
	date-modified = {2023-06-14 09:19:39 -0700},
	journal = {Cognition},
	title = {Making a positive difference: Criticality in groups},
	url = {https://psyarxiv.com/xwm3g/},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/xwm3g/}}

@article{gerstenberg2020physical,
	abstract = {When several causes contributed to an outcome, we often single out one as ``the'' cause. What explains this selection? Previous work has argued that people select abnormal events as causes, though recent work shows that sometimes normal events are preferred over abnormal ones. Existing studies have relied on vignettes that commonly feature agents committing immoral acts. An important challenge to the thesis that norms permeate causal reasoning is that people's responses may merely reflect pragmatic or social reasoning rather than arising from causal cognition per se. We test this hypothesis by asking whether the previously observed patterns of causal selection emerge in tasks that recruit participants' causal reasoning about physical systems. Strikingly, we find that the same patterns observed in vignette studies with intentional agents arises in visual animations of physical interactions. Our results demonstrate how deeply normative expectations affect causal cognition.},
	author = {Tobias Gerstenberg and Thomas F. Icard},
	date-added = {2022-06-27 14:12:02 +0100},
	date-modified = {2022-06-27 14:12:02 +0100},
	journal = {Journal of Experimental Psychology: General},
	number = {3},
	pages = {599--607},
	title = {Expectations affect physical causation judgments},
	volume = {149},
	year = {2020}}

@article{sosa2021dynamics,
	abstract = {When holding others morally responsible, we care about what they did, and what they thought. Traditionally, research in moral psychology has relied on vignette studies, in which a protagonist's actions and thoughts are explicitly communicated. While this research has revealed what variables are important for moral judgment, such as actions and intentions, it is limited in providing a more detailed understanding of exactly how these variables affect moral judgment. Using dynamic visual stimuli that allow for a more fine-grained experimental control, recent studies have proposed a direct mapping from visual features to moral judgments. We embrace the use of visual stimuli in moral psychology, but question the plausibility of a feature-based theory of moral judgment. We propose that the connection from visual features to moral judgments is mediated by an inference about what the observed action reveals about the agent's mental states, and what causal role the agent's action played in bringing about the outcome. We present a computational model that formalizes moral judgments of agents in visual scenes as computations over an intuitive theory of physics combined with an intuitive theory of mind. We test the model's quantitative predictions in three experiments across a wide variety of dynamic interactions between agent and patient.},
	author = {Sosa, Felix A and Ullman, Tomer and Tenenbaum, Joshua B and Gershman, Samuel J and Gerstenberg, Tobias},
	date-added = {2022-06-27 13:53:44 +0100},
	date-modified = {2022-06-27 13:53:59 +0100},
	journal = {Cognition},
	pages = {104890},
	publisher = {Elsevier},
	title = {Moral dynamics: Grounding moral judgment in intuitive physics and intuitive psychology},
	volume = {217},
	year = {2021}}

@article{gerstenberg2021csm,
	abstract = {How do people make causal judgments about physical events? We introduce the counterfactual simulation model (CSM) which predicts causal judgments in physical settings by comparing what actually happened with what would have happened in relevant counterfactual situations. The CSM postulates different aspects of causation that capture the extent to which a cause made a difference to whether and how the outcome occurred, and whether the cause was sufficient and robust. We test the CSM in several experiments in which participants make causal judgments about dynamic collision events. A preliminary study establishes a very close quantitative mapping between causal and counterfactual judgments. Experiment 1 demonstrates that counterfactuals are necessary for explaining causal judgments. Participants' judgments differed dramatically between pairs of situations in which what actually happened was identical, but where what would have happened differed. Experiment 2 features multiple candidate causes and shows that participants' judgments are sensitive to different aspects of causation. The CSM provides a better fit to participants' judgments than a heuristic model which uses features based on what actually happened. We discuss how the CSM can be used to model the semantics of different causal verbs, how it captures related concepts such as physical support, and how its predictions extend beyond the physical domain.},
	author = {Tobias Gerstenberg and Noah D. Goodman and David A. Lagnado and Joshua B. Tenenbaum},
	date-added = {2022-06-27 13:51:09 +0100},
	date-modified = {2022-06-27 13:51:09 +0100},
	journal = {Psychological Review},
	number = {6},
	pages = {936--975},
	title = {A counterfactual simulation model of causal judgments for physical events},
	volume = {128},
	year = {2021},
	bdsk-url-1 = {https://psyarxiv.com/7zj94/}}

@article{srivastava2022imitation,
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ``breakthrough'' behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	annote = {Comment: 27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka{\c s}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ram{\'\i}rez, C{\'e}sar Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and Gonz{\'a}lez, Daniel Mosegu{\'\i} and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Mart{\'\i}nez-Plumed, Fernando and Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germ{\'a}n and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-L{\'o}pez, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Sch{\"u}tze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern{\'a}ndez and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Koco{\'n}, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Col{\'o}n, Luis Oliveros and Metz, Luke and {\c S}enel, L{\"u}tfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Andrea, Madotto and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ram{\'\i}rez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw{\k e}drowski, Micha{\l} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ram{\'o}n Risco and Milli{\`e}re, Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th{\'e}o and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Xinran, Zhao and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	date-added = {2022-06-15 10:20:18 -0400},
	date-modified = {2022-06-20 16:24:37 +0100},
	journal = {arXiv},
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	url = {http://arxiv.org/abs/2206.04615},
	urldate = {2022-06-11},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2206.04615}}

@article{outa2022stop,
	abstract = {Human adults can figure out what happened by combining evidence from different sensory modalities, such as vision and sound. How does the ability to integrate multi-modal information develop in early childhood? Inspired by prior computational work and behavioral studies with adults, we examined 3- to 8-year-old children's ability to reason about the physical trajectory of a ball that was dropped into an occluded Plinko box. Children had to infer in which one of three holes the ball was dropped based on visual information (i.e., where the ball landed) and auditory information (i.e., the sounds of the ball colliding with parts of the box). We compare children's responses to the predictions of four computational models. The results suggest that although even the youngest children make systematic judgments rather than randomly guessing, children's ability to integrate visual and auditory evidence continues to develop into late childhood.},
	author = {Joseph Outa and Xi Jia Zhou and Hyowon Gweon and Tobias Gerstenberg},
	date-added = {2022-05-12 09:49:22 -0700},
	date-modified = {2022-05-12 09:53:52 -0700},
	journal = {Cognitive Science Proceedings},
	title = {Stop, children what's that sound? Multi-modal inference through mental simulation},
	year = {2022}}

@article{beller2022looking,
	abstract = {Mental simulation is a powerful cognitive capacity that underlies people's ability to draw inferences about what happened in the past from the present. Recent work suggests that eye-tracking can be used as a window through which one can study the process of mental simulation in intuitive physics tasks. In our experiment, participants have to figure out in which of three holes a ball was dropped in a virtual Plinko box. We develop a computational model of human intuitive physical reasoning in Plinko that runs repeated simulations in a noisy physics simulator in order to infer in which hole the ball was dropped. We evaluate our model's behavior against multiple human data signals: trial judgments, response times, and eye-movement data. We find that a model that sequentially samples simulations while balancing uncertainty and reward best explains the patterns of participant behavior we observe in these three signals.},
	author = {Aaron Beller and Yingchen Xu and Scott Linderman and Tobias Gerstenberg},
	date-added = {2022-05-12 09:49:22 -0700},
	date-modified = {2022-05-12 09:55:10 -0700},
	journal = {Cognitive Science Proceedings},
	title = {Looking into the past: Eye-tracking mental simulation in physical inference},
	year = {2022}}

@article{wu2022close,
	abstract = {How do people make causal judgments about other's decisions? Prior work has argued that judging causation requires going beyond what actually happened and simulating what would have happened in a relevant counterfactual situation. Here, we extend the counterfactual simulation model of causal judgments for physical events, to explain judgments about other agents' decisions. In our experiments, an agent chooses what path to take to reach a goal. In Experiment 1, participants either made hypothetical judgments about whether the agent would succeed were it to take a certain path, or counterfactual judgments about whether the agent would have succeeded had it taken a different path. In Experiment 2, participants made causal judgments about whether the agent succeeded or failed because of the path that it took. Our computational model accurately captured participants' judgments in both experiments and we find that causal judgments are better explained by counterfactuals rather than hypotheticals.},
	author = {Sarah Wu and Shruti Sridhar and Tobias Gerstenberg},
	date-added = {2022-05-12 09:49:22 -0700},
	date-modified = {2022-05-12 09:51:26 -0700},
	title = {That was close! A counterfactual simulation model of causal judgments about decisions},
	year = {{submitted}}}

@article{gong2023active,
	abstract = {Research on causal learning has largely focused on learning and reasoning about contingency data aggregated across discrete observations or experiments. However, this setting represents only the tip of the causal cognition iceberg. A more general problem lurking beneath is that of learning the latent causal structure that connects events and actions as they unfold in continuous time. In this paper, we examine how people actively learn about causal structure in a continuous time setting, focusing on when and where people intervene on the system and how this shapes their learning. Across two experiments, we find that participants' accuracy depends on both the informativeness and evidential complexity of the data they generate. Moreover, intervention choices strike a balance between maximizing expected information and minimizing expected inferential complexity. That is, we find people time and target their interventions to create simple yet informative causal dynamics. We discuss how the continuous-time setting challenges existing computational accounts of active causal learning, and argue that metacognitive awareness of one's inferential limitations plays a critical role for successful learning in the wild.},
	author = {Gong, Tianwei and Gerstenberg, Tobias and Mayrhofer, Ralf and Bramley, Neil R},
	date-added = {2022-04-06 22:33:51 -0700},
	date-modified = {2023-01-31 15:31:27 -0800},
	journal = {Cognitive Psychology},
	pages = {101542},
	title = {Active causal structure learning in continuous time},
	volume = {140},
	year = {2023},
	bdsk-url-1 = {https://osf.io/jg2c5},
	bdsk-url-2 = {https://doi.org/10.31234/osf.io/jg2c5}}

@incollection{smith2024probabilistic,
	author = {Kevin A. Smith and Jessica B. Hamrick and Adam N. Sanborn and Peter W. Battaglia and Tobias Gerstenberg and Tomer D. Ullman and Joshua B. Tenenbaum},
	booktitle = {Bayesian Models of Cognition: Reverse Engineering the Mind},
	date-added = {2022-03-08 23:06:02 -0800},
	date-modified = {2024-12-18 18:26:54 +0100},
	editor = {Tom L. Griffiths and Nick Chater and Joshua B. Tenenbaum},
	publisher = {MIT Press},
	title = {Probabilistic models of physical reasoning},
	year = {2024}}

@article{zhou2023jenga,
	abstract = {From building towers to picking an orange from a stack of fruit, assessing support is critical for successfully interacting with the physical world. But how do people determine whether one object supports another? In this paper we develop the Counterfactual Simulation Model (CSM) of physical support. The CSM predicts that people judge physical support by mentally simulating what would happen to a scene if the object of interest were removed. Three experiments test the model by asking one group of participants to judge what would happen to a tower if one of the blocks were removed, and another group of participants how responsible that block was for the tower's stability. The CSM accurately captures participants' predictions about what would happen by running noisy simulations that incorporate different sources of uncertainty. Participants' responsibility judgments are closely related to counterfactual predictions: the more likely the tower would be predicted to fall if a block were removed, the more responsible this block was judged for the tower's stability. By construing physical support as preventing from falling, the CSM provides a unified account across dynamic and static physical scenes of how causal judgments arise from the process of counterfactual simulation.},
	author = {Liang Zhou and Kevin A. Smith and Joshua B. Tenenbaum and Tobias Gerstenberg},
	date-added = {2022-02-04 08:45:47 -0800},
	date-modified = {2023-01-26 14:48:17 -0800},
	journal = {Journal of Experimental Psychology: General},
	title = {{Mental Jenga: A counterfactual simulation model of causal judgments about physical support}},
	url = {https://psyarxiv.com/4a5uh},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/4a5uh}}

@article{gerstenberg2022hypothetical,
	abstract = {How do people make causal judgments? In this paper, I show that counterfactuals are necessary for explaining causal judgments about  events, and that hypotheticals don't suffice. In two experiments, participants viewed video clips of dynamic interactions between billiard balls. In Experiment 1, participants either made hypothetical judgments about whether ball B would go through the gate if ball A weren't present in the scene, or counterfactual judgments about whether ball B would have gone through the gate if ball A hadn't been present. Because the clips featured a block in front of the gate that sometimes moved and sometimes stayed put, hypothetical and counterfactual judgments came apart. A computational model that evaluates hypotheticals and counterfactuals by running noisy physical simulations accurately captured participants' judgments. In Experiment 2, participants judged whether ball A caused ball B to go through the gate. The results showed a tight fit between counterfactual and causal judgments, whereas hypothetical judgments didn't predict causal judgments. I discuss the implications of this work for theories of causality, and for studying the development of counterfactual thinking in children.},
	author = {Tobias Gerstenberg},
	date-added = {2021-06-14 22:30:22 -0700},
	date-modified = {2022-01-19 22:28:51 -0800},
	title = {What would have happened? Counterfactuals, hypotheticals, and causal judgments},
	url = {https://psyarxiv.com/rsb46},
	year = {2022},
	bdsk-url-1 = {https://psyarxiv.com/rsb46}}

@article{kominsky2021trajectory,
	abstract = {Young children often struggle to answer the question ``what would have happened?'' particularly in cases where the adult-like ``correct'' answer has the same outcome as the event that actually occurred. Previous work has assumed that children fail because they cannot engage in accurate counterfactual simulations. Children have trouble considering what to change and what to keep fixed when comparing counterfactual alternatives to reality. However, most developmental studies on counterfactual reasoning have relied on binary yes/no responses to counterfactual questions about complex narratives and so have only been able to document when these failures occur but not why and how. Here, we investigate counterfactual reasoning in a domain in which specific counterfactual possibilities are very concrete: simple collision interactions. In Experiment 1, we show that 5- to 10-year-old children (recruited from schools and museums in Connecticut) succeed in making predictions but struggle to answer binary counterfactual questions. In Experiment 2, we use a multiple-choice method to allow children to select a specific counterfactual possibility. We find evidence that 4- to 6-year-old children (recruited online from across the United States) do conduct counterfactual simulations, but the counterfactual possibilities younger children consider differ from adult-like reasoning in systematic ways. Experiment 3 provides further evidence that young children engage in simulation rather than using a simpler visual matching strategy. Together, these experiments show that the developmental changes in counterfactual reasoning are not simply a matter of whether children engage in counterfactual simulation but also how they do so.},
	author = {Kominsky, Jonathan F. and Gerstenberg, Tobias and Pelz, Madeline and Sheskin, Mark and Singmann, Henrik and Schulz, Laura and Keil, Frank C.},
	date-added = {2021-05-13 10:01:04 -0700},
	date-modified = {2021-05-13 10:01:04 -0700},
	doi = {10.1037/dev0001140},
	issn = {1939-0599, 0012-1649},
	journal = {Developmental Psychology},
	language = {en},
	number = {2},
	pages = {253--268},
	title = {The trajectory of counterfactual simulation in development},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/dev0001140},
	urldate = {2021-02-05},
	volume = {57},
	year = {2021},
	bdsk-url-1 = {http://doi.apa.org/getdoi.cfm?doi=10.1037/dev0001140},
	bdsk-url-2 = {https://doi.org/10.1037/dev0001140}}

@inproceedings{davis2021fishing,
	abstract = {Humans have a remarkable ability to go beyond the observable. From seeing the current state of our shared kitchen, we can infer what happened and who did it. Prior work has shown how the physical state of the world licenses inferences about the causal history of events, and the agents that participated in these events. Here, we investigate a previously unstudied source of evidence about what happened: social evaluations. In our experiment, we present situations in which a group failed to optimally coordinate their actions. Participants learn how much each agent was blamed for the outcome, and their task is to make inferences about the situation, the agents' actions, as well as the agents' capabilities. We develop a computational model that accurately captures participants' inferences. The model assumes that people blame others by considering what they should have done, and what causal role their action played. By inverting this generative model of blame, people can figure out what happened.},
	author = {Zachary J. Davis and Kelsey R. Allen and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 43rd Annual Conference of the Cognitive Science Society}},
	date-added = {2021-05-13 09:53:43 -0700},
	date-modified = {2021-05-13 09:57:18 -0700},
	title = {Who went fishing? Inferences from social evaluations},
	year = {2021}}

@article{gerstenberg2021omission,
	abstract = {When do people say that an event that didn't happen was a cause? We extend the counterfactual simulation model (CSM) of causal judgment (Gerstenberg, Goodman, Lagnado, & Tenenbaum, 2021) and test it in a series of three experiments that look at people's causal judgments about omissions in dynamic physical interactions. The problem of omissive causation highlights a series of questions that need to be answered in order to give an adequate causal explanation of why something happened: what are the relevant variables, what are their possible values, how are putative causal relationships evaluated, and how is the causal responsibility for an outcome attributed to multiple causes? The CSM predicts that people make causal judgments about omissions in physical interactions by using their intuitive understanding of physics to mentally simulate what would have happened in relevant counterfactual situations. Prior work has argued that normative expectations affect judgments of omissive causation. Here we suggest a concrete mechanism of how this happens: expectations affect what counterfactuals people consider, and the more certain people are that the counterfactual outcome would have been different from what actually happened, the more causal they judge the omission to be. Our experiments show that both the structure of the physical situation as well as expectations about what will happen affect people's judgments.},
	author = {Tobias Gerstenberg and Simon Stephan},
	date-added = {2020-06-11 23:23:18 -0700},
	date-modified = {2022-06-27 13:58:16 +0100},
	journal = {Cognition},
	pages = {104842},
	title = {A counterfactual simulation model of causation by omission},
	url = {https://psyarxiv.com/wmh4c/},
	volume = {216},
	year = {2021},
	bdsk-url-1 = {https://psyarxiv.com/wmh4c/}}

@inproceedings{bridgers2020granny,
	abstract = {To evaluate others' actions, we consider action outcomes (e.g., positive or negative) and the actors' underlying intentions (e.g., intentional or accidental). However, we often encounter situ- ations where neither actual outcomes nor intentions provide useful evidence for evaluation but representations of unreal- ized (counterfactual) outcomes matter. Here we ask whether preschool-aged children consider counterfactual outcomes to evaluate whose action was more helpful. When two agents each caught one of two falling apples (one caught it above a trash can and the other above a fruit basket), children chose the former as the one who should be thanked (because otherwise the apple would've fallen into the trash). When the agents caught crushed cans, however, children made the op- posite choice, choosing the agent who caught the can over the fruit basket. Even though preschoolers typically struggle with counterfactuals, children in our task readily engaged in such reasoning in the context of social evaluation.},
	author = {Sophie Bridgers and Chuyi Yang and Tobias Gerstenberg and Hyo Gweon},
	booktitle = {{Proceedings of the 42nd Annual Conference of the Cognitive Science Society}},
	date-added = {2020-05-30 14:05:07 -0700},
	date-modified = {2020-05-30 14:12:58 -0700},
	title = {Whom will Granny thank? Thinking about what could have been informs children's inferences about relative helpfulness},
	year = {2020}}

@inproceedings{beller2020language,
	author = {Aaron Beller and Erin Bennett and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 42nd Annual Conference of the Cognitive Science Society}},
	date-added = {2020-05-30 14:05:00 -0700},
	date-modified = {2020-05-30 14:05:00 -0700},
	title = {The language of causation},
	year = {2020}}

@article{grinfeld2020robust,
	abstract = {How do people judge the degree of causal responsibility that an agent has for the outcomes of her actions? We show that a relatively unexplored factor -- the robustness (or stability) of the causal chain linking the agent's action and the outcome -- influences judgments of causal responsibility of the agent. In three experiments, we vary robustness by manipulating the number of background circumstances under which the action causes the effect, and find that causal responsibility judgments increase with robustness. In the first experiment, the robustness manipulation also raises the probability of the effect given the action. Experiments 2 and 3 control for probability-raising, and show that robustness still affects judgments of causal responsibility. In particular, Experiment 3 introduces an Ellsberg type of scenario to manipulate robustness, while keeping the conditional probability and the skill deployed in the action fixed. Experiment 4, replicates the results of Experiment 3, while contrasting between judgments of causal strength and of causal responsibility. The results show that in all cases, the perceived degree of responsibility (but not of causal strength) increases with the robustness of the action-outcome causal chain.},
	author = {Grinfeld, Guy and Lagnado, David and Gerstenberg, Tobias and Woodward, James F. and Usher, Marius},
	date-added = {2020-05-27 13:47:09 -0700},
	date-modified = {2020-05-27 13:47:09 -0700},
	doi = {10.3389/fpsyg.2020.01069},
	issn = {1664-1078},
	journal = {Frontiers in Psychology},
	pages = {1069},
	title = {Causal Responsibility and Robust Causation},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2020.01069},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://www.frontiersin.org/article/10.3389/fpsyg.2020.01069},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2020.01069}}

@article{niemi2020moral,
	abstract = {Prior work has found that moral values that build and bind groups - that is, the binding values of ingroup loyalty, respect for authority, and preservation of purity\textemdash are linked to blaming people who have been harmed. The present research investigated whether people's endorsement of binding values predicts their assignment of the causal locus of harmful events to the victims of the events. We used an implicit causality task from psycholinguistics in which participants read a sentence in the form 'SUBJECT verbed OBJECT because. . .' where male and female proper names occupy the SUBJECT and OBJECT position. The participants were asked to predict the pronoun that follows 'because' - the referent to the subject or object - which indicates their intuition about the likely cause of the event. We also collected explicit judgments of causal contributions and measured participants' moral values to investigate the relationship between moral values and the causal interpretation of events. Using two verb sets and two independent replications (N = 459, 249, 788), we found that greater endorsement of binding values was associated with a higher likelihood of selecting the object as the cause for harmful events in the implicit causality task, a result consistent with, and supportive of, previous moral psychological work on victim blaming. Endorsement of binding values also predicted explicit causal attributions to victims. Overall, these findings indicate that moral values that support the group rather than the individual reliably predict that people shift the causal locus of harmful events to those affected by the harms.},
	author = {Niemi, Laura and Hartshorne, Joshua and Gerstenberg, Tobias and Stanley, Matthew and Young, Liane},
	date-added = {2020-05-24 13:38:10 -0700},
	date-modified = {2020-05-24 13:40:03 -0700},
	doi = {10.1111/cogs.12838},
	issn = {0364-0213, 1551-6709},
	journal = {Cognitive Science},
	language = {en},
	number = {6},
	pages = {e12838},
	title = {Moral values revealt the causality implicit in verb meaning},
	volume = {44},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1111/cogs.12838}}

@article{kirfel2022inference,
	abstract = {What do we communicate with causal explanations? Upon being told, "E because C", a person might learn that C and E both occurred, and perhaps that there is a causal relationship between C and E. In fact, causal explanations systematically disclose much more than this basic information. Here, we offer a communication-theoretic account of explanation that makes speci!c predictions about the kinds of inferences people draw from others' explanations. We test these predictions in a case study involving the role of norms and causal structure. In Experiment 1, we demonstrate that people infer the normality of a cause from an explanation when they know the underlying causal structure. In Experiment 2, we show that people infer the causal structure from an explanation if they know the normality of the cited cause. We !nd these patterns both for scenarios that manipulate the statistical and prescriptive normality of events. Finally, we consider how the communicative function of explanations, as highlighted in this series of experiments, may help to elucidate the distinctive roles that normality and causal structure play in causal judgment, paving the way toward a more comprehensive account of causal explanation.},
	author = {Lara Kirfel and Thomas F. Icard and Tobias Gerstenberg},
	date-added = {2020-05-22 15:37:49 -0700},
	date-modified = {2022-01-15 14:42:38 -0800},
	journal = {Journal of Experimental Psychology: General},
	title = {Inference from explanation},
	year = {2022},
	bdsk-url-1 = {https://psyarxiv.com/x5mqc}}

@article{langenhoff2021voting,
	abstract = {The question of how people hold others responsible has motivated decades of theorizing and empirical work. In this paper, we develop and test a computational model that bridges the gap between broad but qualitative framework theories, and quantitative but narrow models. In our model, responsibility judgments are the result of two cognitive processes: a dispositional inference about a person's character from their action, and a causal attribution about the person's role in bringing about the outcome. We test the model in a group setting in which political committee members vote on whether or not a policy should be passed. We assessed participants' dispositional inferences and causal attributions by asking how surprising and important a committee member's vote was. Participants' answers to these questions in Experiment 1 accurately predicted responsibility judgments in Experiment 2. In Experiments 3 and 4, we show that the model also predicts moral responsibility judgments, and that importance matters more for responsibility, while surprise matters more for judgments of wrongfulness.},
	author = {Antonia F Langenhoff and Alexander Wiegmann and Joseph Y Halpern and Joshua B Tenenbaum and Tobias Gerstenberg},
	date-added = {2019-09-19 17:00:26 -0700},
	date-modified = {2022-06-27 16:01:29 +0100},
	journal = {Cognitive Psychology},
	pages = {101412},
	title = {Predicting responsibility judgments from dispositional inferences and causal attributions},
	volume = {129},
	year = {2021}}

@incollection{bramley2019intervening,
	abstract = {Much of what we know about the world comes from acting on it, and observing the consequences of our actions. In the literature, causal learning from interventions and from observing temporal dynamics have largely received separate attention due to the different datasets they are usually applied to. However, we argue that in human cognition, interventions and temporal dynamics are inseparable. We trace how causal inference tools developed in data science have been applied to understanding human causal learning and reasoning, highlight the current shortcomings of both intervention-based and time-based approaches taken separately, and describe recent work that starts to bring the two together. We end by sketching an account of interventional and temporal evidence as constituents of a unified online causal learning process.},
	author = {Neil R. Bramley and Tobias Gerstenberg and Ralf Mayrhofer and David A. Lagnado},
	booktitle = {Time and Causality Across the Sciences},
	date-added = {2019-08-19 14:47:31 -0700},
	date-modified = {2019-08-19 14:47:35 -0700},
	editor = {Samantha Kleinberg},
	pages = {86--115},
	publisher = {Cambridge University Press},
	title = {Intervening in time},
	year = {2019}}

@inproceedings{kominsky2019trajectory,
	abstract = {Previous work has argued that young children do not answer counterfactual questions (e.g. ``what would have happened?'') by constructing simulations of alternative possibilities in the way adults do. Here, we propose that children can engage in simulation when answering these questions, but consider different counterfactual possibilities than adults. While most previous research has relied on narrative stimuli, we use causal perception events, which are understood even in infancy. In Experiment 1, we replicate earlier findings that children struggle with counterfactual reasoning, but show that they are capable of conducting the required simulations in a prediction task. In Experiment 2, we use a novel multiple-choice method that allows us to study not only when children get it right, but also how they get it wrong. We find evidence that 4-year-olds engage in simulation, but preserve only some features of what actually happened and not others.},
	address = {Montreal, QB},
	author = {Jonathan F. Kominsky and Tobias Gerstenberg and Madeline Pelz and Henrik Singmann and Mark Sheskin and Frank Keil},
	booktitle = {{Proceedings of the 41st Annual Conference of the Cognitive Science Society}},
	date-added = {2019-08-07 14:47:12 -0700},
	date-modified = {2019-08-07 14:47:12 -0700},
	editor = {A.K. Goel and C.M. Seifert and C. Freksa},
	pages = {2044--2050},
	publisher = {Cognitive Science Society},
	title = {The trajectory of counterfactual simulation in development},
	year = {2019}}

@article{morris2019quantitative,
	abstract = {When many events contributed to an outcome, people consistently judge some more causal than others, based in part on the prior probabilities of those events. For instance, when a tree bursts into flames, people judge the lightning strike more of a cause than the presence of oxygen in the air---in part because oxygen is so common, and lightning strikes are so rare. These effects, which play a major role in several prominent theories of token causation, have largely been studied through qualitative manipulations of the prior probabilities. Yet, there is good reason to think that people's causal judgments are on a continuum---and relatively little is known about how these judgments vary quantitatively as the prior probabilities change. In this paper, we measure people's causal judgment across parametric manipulations of the prior probabilities of antecedent events. Our experiments replicate previous qualitative findings, and also reveal several novel patterns that are not well-described by existing theories.
},
	author = {Adam Morris and Jonathan Phillips and Tobias Gerstenberg and Fiery Cushman},
	date-added = {2019-08-06 15:03:07 -0700},
	date-modified = {2019-08-06 15:03:07 -0700},
	journal = {{PLoS ONE}},
	number = {8},
	pages = {e0219704},
	title = {Quantitative causal selection patterns in token causation},
	volume = {14},
	year = {2019},
	bdsk-url-1 = {https://psyarxiv.com/upv8t}}

@article{yildirim2019difficulty,
	abstract = {How do we estimate the difficulty of performing a new task, a task we've never tried before such as making a sculpture, a birthday cake, or building a tower with LEGO blocks? Estimating difficulty helps us appreciate others' accomplishments, and plays a critical role in deciding whether to undertake new tasks ourselves. Here we give a computational account of how humans judge the difficulty of a range of physical construction tasks, whererby the goal is to go from an inital configuration (e.g. blocks scattered on the floor) to a target configuration (e.g. a block tower). Our model takes into account two key aspects that influence construction difficulty: physical effort and physical risk. Physical effort captures the minimal raw work needed to  transport all objects to their final positions and is computed using a hybrid task-and-motion planner. Physical risk corresponds to the precision with which objects must be transported for success, and is computed using noisy physics simulations; it reflects the costs (e.g., attention, coordination and fine motor movements) needed to ensure precise motion. We show that the full effort-risk model captures human estimates of difficulty and construction time better than either component alone, and that difficulty judgments are selectively sensitive to effort and risk in a task-dependent manner.},
	author = {Ilker Yildirim and Basil Saeed and Grace Bennett-Pierre and Tobias Gerstenberg and Joshua B. Tenenbaum and Hyowon Gweon},
	date-added = {2019-05-09 15:21:11 -0700},
	date-modified = {2019-05-09 15:21:11 -0700},
	journal = {{Proceedings of the 41st Annual Conference of the Cognitive Science Society}},
	title = {Explaining intuitive difficulty judgments by modeling physical effort and risk},
	year = {2019}}

@article{bramley2018experiments,
	abstract = {Many aspects of our physical environment are hidden. For example, it is hard to estimate how heavy an object is from visual observation alone. In this paper we examine how people actively "experiment" within the physical world to discover such latent properties. In the first part of the paper, we develop a novel framework for the quantitative analysis of the information produced by physical interactions. We then describe two experiments that present participants with moving objects in "microworlds" that operate according to continuous spatiotemporal dynamics similar to everyday physics (i.e., forces of gravity, friction, etc ...). Participants were asked to interact with objects in the microworlds in order to identify their masses, or the forces of attraction/repulsion that governed their movement. Using our modeling framework, we find that learners who freely interacted with the physical system selectively produced evidence that revealed the physical property consistent with their inquiry goal. As a result, their inferences were more accurate than for passive observers and, in some contexts, for yoked participants who watched video replays of an active learner's interactions. We characterize active learners' actions into a range of micro-experiment strategies and discuss how these might be learned or generalized from past experience. The technical contribution of this work is the development of a novel analytic framework and methodology for the study of interactively learning about the physical world. Its empirical contribution is the demonstration of sophisticated goal directed human active learning in a naturalistic context.},
	author = {Neil R. Bramley and Tobias Gerstenberg and Joshua B. Tenenbaum and Todd M. Gureckis},
	date-added = {2018-06-06 16:01:12 +0000},
	date-modified = {2018-06-06 16:01:12 +0000},
	journal = {Cognitive Psychology},
	pages = {9--38},
	title = {Intuitive experimentation in the physical world},
	volume = {105},
	year = {2018}}

@article{gerstenberg2018expectations,
	abstract = {How do people hold others responsible for the consequences of their actions? We propose a computational model that attributes responsibility as a function of what the observed action reveals about the person, and the causal role that the person's action played in bringing about the outcome. The model first infers what type of person someone is from having observed their action. It then compares a prior expectation of how a person would behave with a posterior expectation after having observed the person's action. The model predicts that a person is blamed for negative outcomes to the extent that the posterior expectation is lower than the prior, and credited for positive outcomes if the posterior is greater than the prior. We model the causal role of a person's action by using a counterfactual model that considers how close the action was to having been pivotal for the outcome. The model captures participants' responsibility judgments to a high degree of quantitative accuracy across three experiments that cover a range of different situations. It also solves an existing puzzle in the literature on the relationship between action expectations and responsibility judgments. Whether an unexpected action yields more or less credit depends on whether the action was diagnostic for good or bad future performance.},
	author = {Gerstenberg, Tobias and Ullman, Tomer D. and Nagel, Jonas and Kleiman-Weiner, Max and Lagnado, David A. and Tenenbaum, Joshua B.},
	date-added = {2018-05-08 21:02:43 +0000},
	date-modified = {2018-05-08 21:03:02 +0000},
	doi = {10.1016/j.cognition.2018.03.019},
	issn = {00100277},
	journal = {Cognition},
	pages = {122-141},
	title = {Lucky or clever? From expectations to responsibility judgments},
	volume = {177},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1016/j.cognition.2018.03.019}}

@article{koskuba2018fair,
	abstract = {How do children reward individual members of a team that has just won or lost a game? We know that from preschool age, children consider agents' performance when allocating reward. Here we assess whether children can go further and appreciate performance in context: The same pattern of performance can contribute to a team outcome in different ways, depending on the underlying rule framework. Two experiments, with three age groups (4/5-year-olds, 6/7-year-olds, and adults), varied performance of team members, with the same performance patterns considered under three different game rules for winning or losing. These three rules created distinct underlying causal structures (additive, conjunctive, disjunctive), for how individual performance affected the overall team outcome. Even the youngest children differentiated between different game rules in their reward allocations. Rather than only rewarding individual performance, or whether the team won/lost, children were sensitive to the team structure and how players' performance contributed to the win/loss under each of the three game rules. Not only do young children consider it fair to allocate resources based on merit, but they are also sensitive to the causal structure of the situation which dictates how individual contributions combine to determine the team outcome.},
	author = {Koskuba, Karla and Gerstenberg, Tobias and Gordon, Hannah and Lagnado, David A. and Schlottmann, Anne},
	date-added = {2018-04-30 15:43:15 +0000},
	date-modified = {2018-04-30 15:43:15 +0000},
	journal = {Cognition},
	pages = {234-248},
	title = {What's fair? How children assign reward to members of teams with differing causal structures},
	volume = {177},
	year = {2018}}

@article{morris2025looking,
	abstract = {Throughout human thought and discourse, we make judgments of how much certain particular events caused others: For instance, we judge that a product sold because of its viral ad campaign more than because of its celebrity endorsement, or vice versa. Yet, the precise functional role of such judgments remains elusive. Why do people make and care about these judgments so much, and why do those judgments follow the patterns they do? We show that core properties of actual causal judgment---its focus on rare antecedents with common counterparts, its centralization of necessity, and its dependence on actual as opposed to hypothetical features of the target situation---make it an ideal candidate for a particular approximate planning algorithm. Specifically, we propose that judgments of whether something (e.g. a viral ad campaign) was the cause of an outcome (e.g. robust sales) in past contexts help cumulatively identify effective ways of bringing about an outcome in general future contexts. We offer a formal account of this process and show how it unites seemingly disparate patterns of actual causal judgment in a common functional role.},
	author = {Adam Morris and Jonathan Phillips and Thomas Icard and Joshua Knobe and Tobias and Fiery Cushman},
	journal = {PsyArXiv},
	title = {Looking back to plan ahead: Causal judgments as a sampling approximation for action effectiveness},
	url = {https://psyarxiv.com/nq53z},
	year = {2015},
	bdsk-url-1 = {https://psyarxiv.com/nq53z}}

@article{gates2018tiptoeing,
	abstract = {Language that describes people in a concise manner may conflict with social norms (e.g., referring to people by their race), presenting a conflict between transferring information efficiently and avoiding offensive language. When a speaker is describing others, we propose that listeners consider the speaker's use or absence of potentially offensive language to reason about the speaker's goals. We formalize this hypothesis in a probabilistic model of polite pragmatic language understanding, and use it to generate predictions about interpretations of utterances in ambiguous contexts, which we test empirically. We find that participants are sensitive to potentially offensive language when resolving ambiguity in reference. These results support the idea that listeners represent conflicts in speakers' goals and use that uncertainty to interpret otherwise underspecified utterances.},
	author = {Monica A Gates and Tess L Veuthey and Michael Henry Tessler and Kevin A Smith and Tobias Gerstenberg and Laurie Bayet and Joshua B Tenenbaum},
	date-added = {2018-02-08 03:43:13 +0000},
	date-modified = {2018-05-15 19:24:53 +0000},
	journal = {{Proceedings of the 40th Annual Conference of the Cognitive Science Society}},
	title = {Tiptoeing around it: Inference from absence in potentially offensive speech},
	year = {2018}}

@inproceedings{gerstenberg2018what,
	abstract = {We introduce a novel experimental paradigm for studying multi-modal integration in causal inference. Our experiments feature a physically realistic Plinko machine in which a ball is dropped through one of three holes and comes to rest at the bottom after colliding with a number of obstacles. We develop a hypothetical simulation model which postulates that people figure out what happened by integrating visual and auditory evidence through mental simulation. We test the model in a series of three experiments. In Experiment 1, participants only receive visual information and either predict where the ball will land, or infer in what hole it was dropped based on where it landed. In Experiment 2, participants receive both visual and auditory information -- they hear what sounds the dropped ball makes. We find that participants are capable of integrating both sources of information, and that the sounds help them figure out what happened. In Experiment 3, we show strong cue integration: even when vision and sound are individually completely non-diagnostic, participants succeed by combining both sources of evidence.},
	author = {Tobias Gerstenberg and Max H. Siegel and Joshua B. Tenenbaum},
	booktitle = {Proceedings of the 40th Annual Conference of the Cognitive Science Society},
	date-added = {2018-02-08 03:43:13 +0000},
	date-modified = {2018-06-12 20:55:55 +0000},
	title = {What happened? Reconstructing the past from vision and sound},
	year = {2018}}

@article{gerstenberg2017tracking,
	abstract = {How do people make causal judgments? What role, if any, does counterfactual simulation play? Counterfactual theories of causal judgments predict that people compare what actually happened with what would have happened if the candidate cause had been absent. Process theories predict that people focus only on what actually happened, to assess the mechanism linking candidate cause and outcome. We tracked participants' eye movements while they judged whether one billiard ball caused another one to go through a gate or prevented it from going through. Both participants' looking patterns and their judgments demonstrated that counterfactual simulation played a critical role. Participants simulated where the target ball would have gone if the candidate cause had been removed from the scene. The more certain participants were that the outcome would have been different, the stronger the causal judgments. These results provide the first direct evidence for spontaneous counterfactual simulation in an important domain of high-level cognition.},
	author = {Tobias Gerstenberg and Matthew F. Peterson and Noah D. Goodman and David A. Lagnado and Joshua B. Tenenbaum},
	date-added = {2018-02-08 03:42:44 +0000},
	date-modified = {2018-02-08 03:44:03 +0000},
	doi = {10.1177/0956797617713053},
	journal = {Psychological Science},
	number = {12},
	pages = {1731--1744},
	publisher = {{SAGE} Publications},
	title = {Eye-Tracking causality},
	url = {https://doi.org/10.1177\%2F0956797617713053},
	volume = {28},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1177%5C%2F0956797617713053},
	bdsk-url-2 = {https://doi.org/10.1177/0956797617713053}}

@inproceedings{gerstenberg2014replacement,
	abstract = {In order to be held responsible, a person's action has to have made some sort of difference to the outcome. In this paper, we propose a counterfactual replacement model according to which people attribute responsibility by comparing their prior expectation about how an agent was going to act in a given situation, with their posterior expectation after having observed the agent's action. The model predicts blame if the posterior expectation is worse than the prior expectation and credit if it is better. In a novel experiment, we manipulate people's prior expectations by changing the framing of a structurally isomorphic task. As predicted by our counterfactual replacement model, people's prior expectations significantly influenced their responsibility attributions. We also show how our model can capture Johnson and Rips's (2013) findings that an agent is attributed less responsibility for bringing about a positive outcome when their action was suboptimal rather than optimal.},
	address = {Austin, TX},
	author = {Tobias Gerstenberg and Tomer D. Ullman and Max Kleiman-Weiner and David A/ Lagnado and Joshua B Tenenbaum},
	booktitle = {{Proceedings of the 36th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {P. Bello and M. Guarini and M. McShane and B. Scassellati},
	pages = {2263--2268},
	publisher = {Cognitive Science Society},
	title = {Wins above replacement: Responsibility attributions as counterfactual replacements},
	year = {2014}}

@inproceedings{mccoy2012probabilistic,
	abstract = {We consider an approach to blame attribution based on counterfactual reasoning in probabilistic generative models. In this view, people intervene on each variable within their model and assign blame in proportion to how much a change to a variable would have improved the outcome. This approach raises two questions: First, what structure do people use to represent a given situation? Second, how do they choose what alternatives to consider when intervening on an event? We use a series of coin-tossing scenarios to compare empirical data to different models within the proposed framework. The results suggest that people sample their intervention values from a prior rather than deterministically switching the value of a variable. The results further suggest that people represent scenarios differently when asked to reason about their own blame attributions, compared with the blame attributions they believe others will assign.},
	author = {McCoy, J. and Ullman, T. D. and Stuhlm\"{u}ller, A. and Gerstenberg, T. and Tenenbaum, J. B.},
	booktitle = {{Proceedings of the 34th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Miyake, N. and Peebles, D. and Cooper, R. P.},
	pages = {1996--2001},
	publisher = {{Austin, TX: Cognitive Science Society}},
	title = {{Why blame Bob? Probabilistic generative models, counterfactual reasoning, and blame attribution}},
	year = {2012}}

@article{gerstenberg2012when,
	abstract = {In two experiments, we established an order effect in responsibility attributions. In line with Spellman (Journal of Experimental Psychology: General 126: 323--348, 1997), who proposed that a person's perceived causal contribution varies with the degree to which it changes the probability of the eventual outcome, Experiment 1 showed that in a team challenge in which the players contribute sequentially, the last player's blame or credit is attenuated if the team's result has already been determined prior to her acting. Experiment 2 illustrated that this attenuation effect does not overgeneralize to situations in which the experienced order of events does not map onto the objective order of events; the level of the last person's performance is only discounted if that person knew that the result was already determined. Furthermore, Experiment 1 demonstrated that responsibility attributions remain sensitive to differences in performance, even if the outcome is already determined. We suggest a theoretical extension of Spellman's model, according to which participants' responsibility attributions are determined not only by whether a contribution made a difference in the actual situation, but also by whether it would have made a difference had things turned out somewhat differently.},
	author = {Gerstenberg, T. and Lagnado, D. A.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Psychonomic Bulletin \& Review},
	number = {4},
	pages = {729--736},
	title = {When contributions make a difference: Explaining order effects in responsibility attributions},
	volume = {19},
	year = {2012}}

@inproceedings{gerstenberg2016almost,
	abstract = {When did something almost happen? In this paper, we investigate what brings counterfactual worlds close. In Experiments 1 and 2, we find that participants' judgments about whether something almost happened are determined by the causal proximity of the alternative outcome. Something almost happened, when a small perturbation to the relevant causal event would have been sufficient to bring it about. In contrast to previous work that has argued that prior expectations are neglected when judging the closeness of counterfactual worlds (Kahneman & Varey, 1990), we show in Experiment 3 that participants are more likely to say something almost happened when they did not expect it. Both prior expectations and causal distance influence judgments of ``almost''. In Experiment 4, we show how both causal proximity and beliefs about what would have happened in the absence of the cause jointly explain judgments of ``almost caused'' and ``almost prevented''.},
	address = {Austin, TX},
	author = {Tobias Gerstenberg and Joshua B. Tenenbaum},
	booktitle = {{Proceedings of the 38th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {A Papafragou and D Grodner and D Mirman and J C Trueswell},
	pages = {2777--2782},
	publisher = {Cognitive Science Society},
	title = {Understanding ``almost'': Empirical and computational studies of near misses},
	year = {2016}}

@inproceedings{bramley2014order,
	abstract = {The timing and order in which a set of events occur strongly influences whether people judge them to be causally related. But what do people think particular temporal patterns of events tell them about causal structure? And how do they integrate multiple pieces of temporal evidence? We present a behavioral experiment that explores human causal structure induction from multiple temporal patterns of observations. We compare two simple Bayesian models that make no assumptions about delay lengths, assume that causes must precede their effects but differ in whether they assume simultaneous events can also be causally connected. We find that participants' judgments are in line with the model that rules out simultaneous causation. Variants of this model that assume people update their beliefs conservatively provide a close fit to participants' judgments. We discuss possible psychological bases for this conservative belief updating and how we plan to further explore how people learn about causal structure from time.},
	address = {Austin, TX},
	author = {Neil R. Bramley and Tobias Gerstenberg and David A. Lagnado},
	booktitle = {{Proceedings of the 36th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {P. Bello and M. Guarini and M. McShane and B. Scassellati},
	pages = {236--241},
	publisher = {Cognitive Science Society},
	title = {The order of things: Inferring causal structure from temporal patterns},
	year = {2014}}

@inproceedings{gerstenberg2010dice,
	abstract = {How much are people's responsibility attributions affected by intended versus actual contributions in group contexts? A novel experimental-game paradigm dissociated intended from actual contributions: good intentions could result in bad outcomes and bad intentions in good ones. Participants acted as external judges and attributed responsibility to computer players engaging in a repeated game. On each round, three players formed a group and each chose to roll one of three dice that differed in terms of price and probability distribution. The team won if the sum exceeded a certain threshold. The results showed that both intended contribution, reflected in the choice of die, and actual contribution, reflected in the outcome of rolling the die, were determinants of participants' responsibility attributions. However, contrary to previous evidence (Cushman, Dreber, Wang, & Costa, 2009), more participants based their attributions on the intention rather than the outcome.},
	author = {Gerstenberg, T. and Lagnado, D. A. and Kareev, Y.},
	booktitle = {{Proceedings of the 32nd Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Ohlsson, S. and Catrambone, R.},
	pages = {1697--1702},
	publisher = {{A}ustin, {TX}: Cognitive {S}cience {S}ociety},
	title = {The dice are cast: The role of intended versus actual contributions in responsibility attribution},
	year = {2010}}

@article{gerstenberg2009allocation,
	abstract = {How do people assign responsibility to an individual cause or person in a situation of collective responsibility? This dissertation addresses the question by designing an experimental game, the Triangle Game (TG), which participants play in a group with three computer players. The participants' task is to count triangles presented in complex diagrams for a brief period of time. Whether the group wins or loses depends on the accuracy of each player in the group. After each round, participants assign responsibility for the result to each player. Three experimental conditions differ in how the individual judgments are combined to determine the deviation of the group's answer from the correct solution. This deviation determines whether a round is lost or won. For the three experimental conditions, the group's deviation is determined, respectively, by the sum of each player's deviation, the deviation of the least accurate player or the deviation of the most accurate player. The results show that these differences in the underlying causal structure have a significant influence on participants' responsibility ratings. Furthermore, the predictions of different cognitive models are tested. A model for assigning responsibility to individual causes in cases of multiple causation developed by Chockler and Halpern (2003) describes the empirical data best. A second experiment replicates these findings. In an additional step, participants were asked to change the result of each round, for example from a loss to a win, by minimally altering the answers that the players had given in that round. The results show that participants perceive a possible world, where several small changes have been made as more similar to the actual world than a possible world with one big change. The implications of these findings for counterfactual theories of causation are discussed. The main advantage of the TG is that it allows testing psychological theories of responsibility attribution in a formally rigorous manner.},
	author = {Gerstenberg, T.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Unpublished MSc thesis},
	title = {The allocation of responsibility amongst multiple causes},
	year = {2009}}

@article{gerstenberg2010spreading,
	abstract = {How do people assign responsibility to individuals in a group context? Participants played a repeated trial experimental game with three computer players, in which they counted triangles presented in complex diagrams. Three between-subject conditions differed in how the group outcome was computed from the individual players' answers. After each round, participants assigned responsibility for the outcome to each player. The results showed that participants' assignments varied between conditions, and were sensitive to the function that translated individual contributions into the group outcome. The predictions of different cognitive models of attribution were tested, and the Structural Model (Chockler & Halpern, 2004) predicted the data best.},
	author = {Gerstenberg, T. and Lagnado, D. A.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Cognition},
	number = {1},
	pages = {166--171},
	publisher = {Elsevier},
	title = {Spreading the blame: The allocation of responsibility amongst multiple agents},
	volume = {115},
	year = {2010}}

@inproceedings{gerstenberg2015voting,
	abstract = {How do people assign responsibility for the outcome of an election? In previous work, we have shown that responsibility judgments in achievement contexts are affected by the probability that a person's contribution is necessary, and by how close it was to being pivotal (Lagnado, Gerstenberg, & Zultan, 2013). Here we focus on responsibility judgments in voting scenarios. We varied the number of people in different voting committees, their political affiliations, the number of votes required for a policy to pass, which party supports the policy, and the pattern of votes (creating 170 different situations). As expected, we found that participants' responsibility judgments increased the closer the voter was to being pivotal. Further, judgments increased the more unexpected a vote was. Voters were assigned more responsibility when they voted against the majority in the committee, and when they voted against their party affiliation.},
	address = {Austin, TX},
	author = {Gerstenberg, T. and Halpern, J. Y. and Tenenbaum, J. B.},
	booktitle = {{Proceedings of the 37th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Noelle, D. C. and Dale, R. and Warlaumont, A. S. and Yoshimi, J. and Matlock, T., Jennings and C. D. and Maglio, P. P.},
	pages = {788--793},
	publisher = {Cognitive Science Society},
	title = {Responsibility judgments in voting scenarios},
	year = {2015}}

@inproceedings{gerstenberg2011rational,
	abstract = {Two experiments establish a rational order effect in responsibility attributions. Experiment 1 shows that in a team challenge in which players contribute sequentially, the last player's blame or credit for a performance is reduced if the team's result is already determined prior to his acting. However, credit and blame attributions still vary with quality of performance in these cases. This finding is at odds with Spellman (1997) who proposed that a person's perceived contribution varies with the degree to which it changes the probability of the eventual outcome. Experiment 2 illustrates that the rational order effect does not overgeneralize to situations in which the experienced order of events does not map onto the objective order of events. The quality of the last person's performance is only discredited if she knew that the result was already determined.},
	author = {Gerstenberg, T. and Lagnado, D. A. and Speekenbrink, M. and Cheung, C.},
	booktitle = {Proceedings of the 33rd {A}nnual {C}onference of the {C}ognitive {S}cience {S}ociety.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Carlson, C. and H{\"o}lscher, C. and Shipley, T.},
	pages = {1715--1720},
	publisher = {{A}ustin, {TX}: Cognitive {S}cience {S}ociety},
	title = {Rational order effects in responsibility attributions},
	year = {2011}}

@article{gershman2016goals,
	abstract = {Human success and even survival depends on our ability to predict what others will do by guessing what they are thinking. If I accelerate, will he yield? If I propose, will she accept? If I confess, will they forgive? Psychologists call this capacity ``theory of mind.'' According to current theories, we solve this problem by assuming that others are rational actors. That is, we assume that others design and execute efficient plans to achieve their goals, given their knowledge. But if this view is correct, then our theory of mind is startlingly incomplete. Human action is not always a product of rational planning, and we would be mistaken to always interpret others' behaviors as such. A wealth of evidence indicates that we often act habitually---a form of behavioral control that depends not on rational planning, but rather on a history of reinforcement. We aim to test whether the human theory of mind includes a theory of habitual action and to assess when and how it is deployed. In a series of studies, we show that human theory of mind is sensitive to factors influencing the balance between habitual and planned behavior.},
	author = {Gershman, S. J. and Gerstenberg, T. and Baker, C. L. and Cushman, F.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {PLoS ONE},
	number = {9},
	pages = {e0162246},
	title = {Plans, habits, and theory of mind},
	volume = {11},
	year = {2016}}

@inproceedings{gerstenberg2012pingpong,
	abstract = {How do people make inferences from complex patterns of evidence across diverse situations? What does a computational model need in order to capture the abstract knowledge people use for everyday reasoning? In this paper, we explore a novel modeling framework based on the probabilistic language of thought (PLoT) hypothesis, which conceptualizes thinking in terms of probabilistic inference over compositionally structured representations. The core assumptions of the PLoT hypothesis are realized in the probabilistic programming language Church (Goodman, Mansinghka, Roy, Bonawitz, & Tenenbaum, 2008). Using ``ping pong tournaments'' as a case study, we show how a single Church program concisely represents the concepts required to specify inferences from diverse patterns of evidence. In two experiments, we demonstrate a very close fit between our model's predictions and participants' judgments. Our model accurately predicts how people reason with confounded and indirect evidence and how different sources of information are integrated.},
	author = {Gerstenberg, T. and Goodman, N. D.},
	booktitle = {{Proceedings of the 34th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Miyake, N. and Peebles, D. and Cooper, R. P.},
	pages = {1590--1595},
	publisher = {{Austin, TX: Cognitive Science Society}},
	title = {{Ping Pong in Church: Productive use of concepts in human probabilistic inference}},
	year = {2012}}

@inproceedings{yildirim2017problem,
	abstract = {In this paper, we present a new task that investigates how people interact with and make judgments about towers of blocks. In Experiment 1, participants in the lab solved a series of problems in which they had to re-configure three blocks from an initial to a final configuration. We recorded whether they used one hand or two hands to do so. In Experiment 2, we asked participants online to judge whether they think the person in the lab used one or two hands. The results revealed a close correspondence between participants' actions in the lab, and the mental simulations of participants online. To explain participants' actions and mental simulations, we develop a model that plans over a symbolic representation of the situation, executes the plan using a geometric solver, and checks the plan's feasibility by taking into account the physical constraints of the scene. Our model explains participants' actions and judgments to a high degree of quantitative accuracy.},
	address = {Austin, TX},
	author = {Ilker Yildirim and Tobias Gerstenberg and Basil Saeed and Marc Toussant and Joshua B. Tenenbaum},
	booktitle = {{Proceedings of the 39th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Glenn Gunzelmann and Andrew Howes and Thora Tenbrink and Eddy Davelaar},
	pages = {3584--3589},
	publisher = {Cognitive Science Society},
	title = {Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints},
	year = {2017}}

@article{meder2010observing,
	abstract = {Recently, a number of rational theories have been put forward which provide a coherent formal framework for modeling different types of causal inferences, such as prediction, diagnosis, and action planning. A hallmark of these theories is their capacity to simultaneously express probability distributions under observational and interventional scenarios, thereby rendering it possible to derive precise predictions about interventions (``doing'') from passive observations (``seeing''). In Part 1 of the paper we discuss different modeling approaches for formally representing interventions and review the empirical evidence on how humans draw causal inferences based on observations or interventions. We contrast deterministic interventions with imperfect actions yielding unreliable or unknown outcomes. In Part 2, we discuss alternative strategies for making interventional decisions when the causal structure is unknown to the agent. A Bayesian approach of rational causal inference, which aims to infer the structure and its parameters from the available data, provides the benchmark model. This account is contrasted with a heuristic approach which knows categories of causes and effects but neglects further structural information. The results of computer simulations show that despite its computational parsimony the heuristic approach achieves very good performance compared to the Bayesian model.},
	author = {Meder, B. and Gerstenberg, T. and Hagmayer, Y. and Waldmann, M. R.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Open Psychology Journal},
	pages = {119--135},
	title = {Observing and Intervening: Rational and Heuristic Models of Causal Decision Making},
	volume = {3},
	year = {2010}}

@inproceedings{gerstenberg2012noisy,
	abstract = {There is a long tradition in both philosophy and psychology to separate process accounts from dependency accounts of causation. In this paper, we motivate a unifying account that explains people's causal attributions in terms of counterfactuals defined over probabilistic generative models. In our experiments, participants see two billiard balls colliding and indicate to what extent ball A caused/prevented ball B to go through a gate. Our model predicts that people arrive at their causal judgments by comparing what actually happened with what they think would have happened, had the collision between A and B not taken place. Participants' judgments about what would have happened are highly correlated with a noisy model of Newtonian physics. Using those counterfactual judgments, we can predict participants' cause and prevention judgments very accurately (r = .99). Our framework also allows us to capture intrinsically counterfactual judgments such as almost caused/prevented.},
	author = {Gerstenberg, T. and Goodman, N. D. and Lagnado, D. A. and Tenenbaum, J. B.},
	booktitle = {{Proceedings of the 34th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Miyake, N. and Peebles, D. and Cooper, R. P.},
	pages = {378--383},
	publisher = {{Austin, TX: Cognitive Science Society}},
	title = {{Noisy Newtons: Unifying process and dependency accounts of causal attribution}},
	year = {2012}}

@inproceedings{bramley2016natural,
	abstract = {In this paper, we bring together research on active learning and intuitive physics to explore how people learn about ``microworlds'' with continuous spatiotemporal dynamics. Participants interacted with objects in simple two-dimensional worlds governed by a physics simulator, with the goal of identifying latent physical properties such as mass, and forces of attraction or repulsion. We find an advantage for active learners over passive and yoked controls. Active participants spontaneously performed several kinds of ``natural experiments'' which reveal the objects' properties with varying success. While yoked participants' judgments were affected by the quality of the active participant they observed, they did not share the learning advantage, performing no better than passive controls overall. We discuss possible explanations for the divergence between active and yoked learners, and outline further steps to categorize and explore active learning in the wild.},
	address = {Austin, TX},
	author = {Neil R. Bramley and Tobias Gerstenberg and Joshua B. Tenenbaum},
	booktitle = {{Proceedings of the 38th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {A Papafragou and D Grodner and D Mirman and J C Trueswell},
	pages = {2567--2572},
	publisher = {Cognitive Science Society},
	title = {Natural science: Active learning in dynamic physical microworlds},
	year = {2016}}

@inproceedings{stephan2017marbles,
	abstract = {Consider the following causal explanation: The ball went through the goal because the defender didn't block it. There are at least two problems with citing omissions as causal explanations. First, how do we choose the relevant candidate omission (e.g. why the defender and not the goalkeeper). Second, how do we determine what would have happened in the relevant counterfactual situation (i.e. maybe the shot would still have gone through the goal even if it had been blocked). In this paper, we extend the counterfactual simulation model (CSM) of causal judgment (Gerstenberg, Goodman, Lagnado, & Tenenbaum, 2014) to handle the second problem. In two experiments, we show how people's causal model of the situation affects their causal judgments via influencing what counterfactuals they consider. Omissions are considered causes to the extent that the outcome in the relevant counterfactual situation would have been different from what it actually was.},
	address = {Austin, TX},
	author = {Simon Stephan and Pascale Willemsen and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 39th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2018-03-26 20:27:47 +0000},
	editor = {Glenn Gunzelmann and Andrew Howes and Thora Tenbrink and Eddy Davelaar},
	pages = {1132--1137},
	publisher = {Cognitive Science Society},
	title = {Marbles in inaction: Counterfactual simulation and causation by omission},
	year = {2017}}

@article{gerstenberg2013making,
	abstract = {In this thesis, I develop a general framework of how people attribute responsibility. In this framework, people's responsibility attributions are modelled in terms of counterfactuals defined over a causal representation of the situation. A person is predicted to be held responsible to the extent that their action made a difference to the outcome. Accordingly, when attributing responsibility we compare what actually happened with the outcome in a simulated counterfactual world in which the person's action had been different. However, a person can still be held responsible for an outcome even if their action made no difference in the actual situation. Responsibility attributions are sensitive to whether a person's action would have made a difference in similar counterfactual situations. Generally, responsibility decreases with the number of events that would have needed to change from the actual situation in order to generate a counterfactual situation in which the person's action would have been pivotal. In addition to how close a person was to being pivotal, responsibility attributions are influenced by how critical a person's action was perceived prior to the outcome. The predictions derived from this general framework are tested in a series of experiments that manipulate a person's criticality and pivotality by varying the causal structure of the situation and the person's mental states. The results show that responsibility between the members of a group diffuses according to the causal structure which determines how individual contributions combine to yield a joint outcome. Differences in the group members' mental states, such as their knowledge about the situation, their expectations about each other's performance as well as their intentions, also affect attributions. Finally, I demonstrate how this general framework can be extended to model attributions for domains in which people have rich, intuitive theories that go beyond what can be expressed with simple causal models.},
	author = {Tobias Gerstenberg},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Unpublished PhD thesis},
	school = {University College London},
	title = {Making a difference: Responsibility, causality, and counterfactuals},
	year = {2013}}

@article{bramley2018time,
	author = {Neil R. Bramley and Tobias Gerstenberg and Ralf Mayrhofer and David A. Lagnado},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2019-02-14 15:07:30 -0800},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	number = {12},
	pages = {1880--1910},
	title = {Time in causal structure learning},
	volume = {44},
	year = {2018}}

@incollection{gerstenberg2017theories,
	abstract = {One of the hallmarks of human intelligence is its flexibility. While computers have achieved better-than-human performance in various games, such as Chess, Jeopardy, or Go, there is no single algorithm yet that works in all of these cases. Humans, however, can excel at all of these games, and many other tasks. We believe that bridging the gap between human and machine intelligence requires two key insights from cognitive science: (i) human knowledge is organized in terms of richly structured intuitive theories, and (ii) many cognitive processes can be understood as causal inferences operating over these structures. In this chapter, we first explain what intuitive theories are, how we can model them as probabilistic, generative programs, and how intuitive theories support various cognitive functions such as prediction, counterfactual reasoning, and explanation. We focus on two domains of knowledge: people's intuitive understanding of physics, and their intuitive understanding of psychology. We show how causal judgments can be modeled as counterfactual contrasts operating over an intuitive theory of physics, and how explanations of an agent's behavior are grounded in a rational planning model that is inverted to infer the agent's beliefs, desires, and abilities. We conclude by highlighting some of the challenges that the intuitive theories framework faces, such as understanding how intuitive theories are learned and developed.},
	author = {Tobias Gerstenberg and Joshua B. Tenenbaum},
	booktitle = {Oxford Handbook of Causal Reasoning},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Michael Waldmannn},
	pages = {515--548},
	publisher = {Oxford University Press},
	title = {Intuitive Theories},
	year = {2017}}

@inproceedings{kleiman-weiner2015intention,
	abstract = {The actions of a rational agent reveal information about its mental states. These inferred mental states, particularly the agent's intentions, play an important role in the evaluation of moral permissibility. While previous computational models have shown that beliefs and desires can be inferred from behavior under the assumption of rational action they have critically lacked a third mental state, intentions. In this work, we develop a novel formalism for intentions and show how they can be inferred as counterfactual contrasts over influence diagrams. This model is used to quantitatively explain judgments about intention and moral permissibility in classic and novel trolley problems.},
	address = {Austin, TX},
	author = {Kleiman-Weiner, M. and Gerstenberg, T. and Levine, S. and Tenenbaum, J. B.},
	booktitle = {{Proceedings of the 37th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Noelle, D. C. and Dale, R. and Warlaumont, A. S. and Yoshimi, J. and Matlock, T., Jennings and C. D. and Maglio, P. P.},
	pages = {1123--1128},
	publisher = {Cognitive Science Society},
	title = {Inference of intention and permissibility in moral decision making},
	year = {2015}}

@inproceedings{niemi2016implicit,
	abstract = {Moral judgment often involves pinning causation for harm to a particular person. Since it reveals ``who one sides with'', expression of moral judgment can be a costly social act that people may be motivated to conceal. Here, we demonstrate that a simple, well-studied psycholinguistic task (implicit causality) can be leveraged as a novel implicit measure of morally relevant causal attributions. Participants decided whether to continue sentences like ``Amy killed Bob because...'' with either the pronoun he or she. We found that (1) implicit causality selections predicted explicit causal judgments, (2) selecting the object (victim) for harm/force events (e.g., kill, rape) predicted endorsement of moral values previously linked to victim-blame, and (3) higher hostile sexism predicted selecting the female as the cause in male-on-female harm/force. The implicit causality task is a new measure of morally motivated causal attribution that may circumvent social desirability concerns.},
	address = {Austin, TX},
	author = {Laura Niemi and Joshua Hartshorne and Tobias Gerstenberg and Liane Young},
	booktitle = {{Proceedings of the 38th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {A Papafragou and D Grodner and D Mirman and J C Trueswell},
	pages = {1745--1750},
	publisher = {Cognitive Science Society},
	title = {Implicit measurement of motivated causal attribution},
	year = {2016}}

@inproceedings{gerstenberg2015how,
	abstract = {How do people make causal judgments? Here, we propose a counterfactual simulation model (CSM) of causal judgment that unifies different views on causation. The CSM predicts that people's causal judgments are influenced by whether a candidate cause made a difference to whether the outcome occurred as well as to how it occurred. We show how whethercausation and how-causation can be implemented in terms of different counterfactual contrasts defined over the same intuitive generative model of the domain. We test the model in an intuitive physics domain where people make judgments about colliding billiard balls. Experiment 1 shows that participants' counterfactual judgments about what would have happened if one of the balls had been removed, are well-explained by an approximately Newtonian model of physics. In Experiment 2, participants judged to what extent two balls were causally responsible for a third ball going through a gate or missing the gate. As predicted by the CSM, participants' judgments increased with their belief that a ball was a whether-cause, a how-cause, as well as sufficient for bringing about the outcome.},
	address = {Austin, TX},
	author = {Gerstenberg, T. and Goodman, N. D. and Lagnado, D. A. and Tenenbaum, J. B.},
	booktitle = {{Proceedings of the 37th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Noelle, D. C. and Dale, R. and Warlaumont, A. S. and Yoshimi, J. and Matlock, T., Jennings and C. D. and Maglio, P. P.},
	pages = {782--787},
	publisher = {Cognitive Science Society},
	title = {How, whether, why: Causal judgments as counterfactual contrasts},
	year = {2015}}

@inproceedings{allen2015fishing,
	abstract = {Many social judgments hinge on assigning responsibility to individuals for their role in a group's success or failure. Often the group's success depends on every team member acting in a rational way. When someone does not conform to what others expect of them, cooperation breaks down. We present a computational model of responsibility judgments for individuals in a cooperative setting. We test the model in two behavioral experiments where participants were asked to evaluate agents acting in a cooperative, one-shot game. In Experiment 1, we show that participants' action predictions are consistent with a recursive reasoning model. In Experiment 2, we show that people's assignments of blame are influenced by both an agent's presumed rationality, or adherence to an expected policy, as well as the pivotality of the agent's actions, or how close the situation was to one in which the action would have made a difference to the outcome.},
	address = {Austin, TX},
	author = {Allen, K. and Jara-Ettinger, J. and Gerstenberg, T. and Kleiman-Weiner, M. and Tenenbaum, J. B.},
	booktitle = {{Proceedings of the 37th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Noelle, D. C. and Dale, R. and Warlaumont, A. S. and Yoshimi, J. and Matlock, T., Jennings and C. D. and Maglio, P. P.},
	pages = {84--89},
	publisher = {Cognitive Science Society},
	title = {Go fishing! Responsibility judgments when cooperation breaks down},
	year = {2015}}

@inproceedings{gerstenberg2014simulation,
	address = {Austin, TX},
	author = {Tobias Gerstenberg and Noah D. Goodman and David A. Lagnado and Joshua B. Tenenbaum},
	booktitle = {{Proceedings of the 36th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {P. Bello and M. Guarini and M. McShane and B. Scassellati},
	pages = {523--528},
	publisher = {Cognitive Science Society},
	title = {From counterfactual simulation to causal judgment},
	year = {2014}}

@article{zultan2012finding,
	abstract = {Attributions of responsibility play a critical role in many group interactions. This paper explores the role of causal and counterfactual reasoning in blame attributions in groups. We develop a general framework that builds on the notion of pivotality: an agent is pivotal if she could have changed the group outcome by acting differently. In three experiments we test successive refinements of this notion -- whether an agent is pivotal in close possible situations and the number of paths to achieve pivotality. In order to discriminate between potential models, we introduced group tasks with asymmetric structures. Some group members were complements (for the two to contribute to the group outcome it was necessary that both succeed) whereas others were substitutes (for the two to contribute to the group outcome it was sufficient that one succeeds). Across all three experiments we found that people's attributions were sensitive to the number of paths to pivotality. In particular, an agent incurred more blame for a team loss in the presence of a successful complementary peer than in the presence of a successful substitute.},
	author = {Zultan, R. and Gerstenberg, T. and Lagnado, D. A.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Cognition},
	number = {3},
	pages = {429--440},
	title = {Finding fault: Counterfactuals and causality in group attributions},
	volume = {125},
	year = {2012}}

@inproceedings{gerstenberg2017faulty,
	abstract = {In this paper we introduce the hypothetical simulation model (HSM) of physical support. The HSM predicts that people judge physical support by mentally simulating what would happen if the object of interest were removed. Two experiments test the model by asking participants to evaluate the extent to which one brick in a tower is responsible for the rest of the bricks staying on a table. The results of both experiments show a very close correspondence between hypothetical simulations and responsibility judgments. We compare three versions of the HSM which differ in how they model people's uncertainty about what would happen. Participants' selections of which bricks would fall are best explained by assuming that hypothetical interventions only lead to local changes while leaving the rest of the scene unchanged.},
	address = {Austin, TX},
	author = {Tobias Gerstenberg and Liang Zhou and Kevin A. Smith and Joshua B. Tenenbaum},
	booktitle = {{Proceedings of the 39th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Glenn Gunzelmann and Andrew Howes and Thora Tenbrink and Eddy Davelaar},
	pages = {409--414},
	publisher = {Cognitive Science Society},
	title = {Faulty towers: A hypothetical simulation model of physical support},
	year = {2017}}

@incollection{goodman2015concepts,
	abstract = {Knowledge organizes our understanding of the world, determining what we expect given what we have already seen. Our predictive representations have two key properties: they are productive, and they are graded. Productive generalization is possible because our knowledge decomposes into concepts - elements of knowledge that are combined and recombined to describe particular situations. Gradedness is the observable effect of accounting for uncertainty - our knowledge encodes degrees of belief that lead to graded probabilistic predictions. To put this a different way, concepts form a combinatorial system that enables description of many different situations; each such situation specifies a distribution over what we expect to see in the world, given what we have seen. We may think of this system as a probabilistic language of thought (PLoT), in which representations are built from language-like composition of concepts, and the content of those representations is a probability distribution on world states. The purpose of this chapter is to formalize these ideas in computational terms, to illustrate key properties of the PLoT approach with a concrete example, and to draw connections with other views of conceptual structure.},
	author = {Goodman, N. D. and Tenenbaum, J. B. and Gerstenberg, T.},
	booktitle = {The Conceptual Mind: New Directions in the Study of Concepts},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-28 02:33:06 +0000},
	editor = {Eric Margolis and Stephen Lawrence},
	pages = {623--653},
	publisher = {MIT Press},
	title = {Concepts in a probabilistic language of thought},
	year = {2015}}

@incollection{lagnado2017causation,
	abstract = {Causation looms large in legal and moral reasoning. People construct causal models of the social and physical world to understand what has happened, how and why, and to allocate responsibility and blame. This chapter explores people's common-sense notion of causation, and shows how it underpins moral and legal judgments. As a guiding framework it uses the causal model framework (Pearl, 2000) rooted in structural models and counterfactuals, and shows how it can resolve many of the problems that beset standard but-for analyses. It argues that legal concepts of causation are closely related to everyday causal reasoning, and both are tailored to the practical concerns of responsibility attribution. Causal models are also critical when people evaluate evidence, both in terms of the stories they tell to make sense of evidence, and the methods they use to assess its credibility and reliability.},
	author = {Lagnado, D. A. and Gerstenberg, T.},
	booktitle = {Oxford Handbook of Causal Reasoning},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Michael Waldmann},
	pages = {565--602},
	publisher = {Oxford University Press},
	title = {Causation in legal and moral reasoning},
	year = {2017}}

@inproceedings{kominsky2014supersession,
	abstract = {When agents violate norms, they are typically judged to be more of a cause of resulting outcomes. In this study, we suggest that norm violations also reduce the causality of other agents, a novel phenomenon we refer to as ``causal supersession.'' We propose and test a counterfactual reasoning model of this phenomenon in three experiments. Experiment 1 shows that causal judgments of one actor are reduced when another actor violates moral norms, even when the outcome in question is neutral. Experiment 2 shows that this causal supersession effect is dependent on a particular event structure, following a prediction of our counterfactual model. Experiment 3 demonstrates that causal supersession can occur with violation of non-moral norms.},
	address = {Austin, TX},
	author = {Jonathan F. Kominsky and Jonathan Phillips and Joshua Knobe and Tobias Gerstenberg and David A. Lagnado},
	booktitle = {{Proceedings of the 36th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {P. Bello and M. Guarini and M. McShane and B. Scassellati},
	pages = {761--766},
	publisher = {Cognitive Science Society},
	title = {Causal supersession},
	year = {2014}}

@article{kominsky2015superseding,
	abstract = {When agents violate norms, they are typically judged to be more of a cause of resulting outcomes. In this paper, we suggest that norm violations also affect the causality attributed to other agents, a phenomenon we refer to as ``causal superseding.'' We propose and test a counterfactual reasoning model of this phenomenon in four experiments. Experiments 1 and 2 provide an initial demonstration of the causal superseding effect and distinguish it from previously studied effects. Experiment 3 shows that this causal superseding effect is dependent on a particular event structure, following a prediction of our counterfactual model. Experiment 4 demonstrates that causal superseding can occur with violations of non-moral norms. We propose a model of the superseding effect based on the idea of counterfactual sufficiency.},
	author = {Jonathan F. Kominsky and Jonathan Phillips and Tobias Gerstenberg and David A. Lagnado and Joshua Knobe},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Cognition},
	pages = {196--209},
	title = {Causal superseding},
	volume = {137},
	year = {2015}}

@article{lagnado2013causal,
	abstract = {How do people attribute responsibility in situations where the contributions of multiple agents combine to produce a joint outcome? The prevalence of over-determination in such cases makes this a difficult problem for counterfactual theories of causal responsibility. In this article, we explore a general framework for assigning responsibility in multiple agent contexts. We draw on the structural model account of actual causation (e.g., Halpern & Pearl, 2005) and its extension to responsibility judgments (Chockler & Halpern, 2004). We review the main theoretical and empirical issues that arise from this literature and propose a novel model of intuitive judgments of responsibility. This model is a function of both pivotality (whether an agent made a difference to the outcome) and criticality (how important the agent is perceived to be for the outcome, before any actions are taken). The model explains empirical results from previous studies and is supported by a new experiment that manipulates both pivotality and criticality. We also discuss possible extensions of this model to deal with a broader range of causal situations. Overall, our approach emphasizes the close interrelations between causality, counterfactuals, and responsibility attributions.},
	author = {Lagnado, D. A. and Gerstenberg, T. and Zultan, R.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Cognitive Science},
	pages = {1036--1073},
	title = {Causal responsibility and counterfactuals},
	volume = {47},
	year = {2013}}

@inproceedings{bramley2017continuous,
	abstract = {Event timing and interventions are important and intertwined cues to causal structure, yet they have typically been studied separately. We bring them together for the first time in an experiment where participants learn causal structure by performing interventions in continuous time. We contrast learning in acyclic and cyclic devices, with reliable and unreliable cause-effect delays. We show that successful learners use interventions to structure and simplify their interactions with the devices and that we can capture judgment patterns with heuristics based on online construction and testing of a single structural hypothesis.},
	address = {Austin, TX},
	author = {Neil R. Bramley and Ralf Mayrhofer and Tobias Gerstenberg and David A. Lagnado},
	booktitle = {{Proceedings of the 39th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Glenn Gunzelmann and Andrew Howes and Thora Tenbrink and Eddy Davelaar},
	pages = {150--155},
	publisher = {Cognitive Science Society},
	title = {Causal learning from interventions and dynamics in continuous time},
	year = {2017}}

@article{alicke2015conceptions,
	abstract = {Understanding the causes of human behavior is essential for advancing one's interests and for coordinating social relations. The scientific study of how people arrive at such understandings or explanations has unfolded in four distinguishable epochs in psychology, each characterized by a different metaphor that researchers have used to represent how people think as they attribute causality and blame to other individuals. The first epoch was guided by an _intuitive scientist_ metaphor, which emphasized whether observers perceived behavior to be caused by the unique tendencies of the actor or by common reactions to the requirements of the situation. This metaphor was displaced in the second epoch by an _intuitive lawyer_ depiction that focused on the need to hold people responsible for their misdeeds. The third epoch was dominated by theories of counterfactual thinking, which conveyed a _person as reconstructor_ approach that emphasized the antecedents and consequences of imagining alternatives to events, especially harmful ones. With the current upsurge in moral psychology, the fourth epoch emphasizes the moral-evaluative aspect of causal judgment, reflected in a _person as moralist_ metaphor. By tracing the progression from the person-environment distinction in early attribution theories to present concerns with moral judgment, our goal is to clarify how causal constructs have been used, how they relate to one another, and what unique attributional problems each addresses.},
	author = {Alicke, M. D. and Mandel, D. R and Hilton, D. and Gerstenberg, T. and Lagnado, D. A.},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	journal = {Perspectives on Psychological Science},
	number = {6},
	pages = {790--812},
	title = {Causal conceptions in social explanation and moral evaluation: A historical tour},
	volume = {10},
	year = {2015}}

@inproceedings{gerstenberg2011blame,
	abstract = {This study investigates the influence of players' performance and level of skill on responsibility attributions in groups. Participants act as external judges and evaluate the performance of teams of differently skilled players who compete in a darts game. The results show that both performance and skill influence responsibility attributions. Poor performance elicits high blame and low credit ratings and vice versa for good performance. Responsibility attributions to one player did not vary as a function of the other player's performance. The influence of skill on responsibility attributions was asymmetric. While skilled players were blamed more for losses than unskilled players, credit ratings did not vary significantly as a function of skill. This result is in line with people's strong tendency to spontaneously consider upwards counterfactual alternatives for losses over downwards counterfactuals for wins.},
	author = {Gerstenberg, T. and Ejova, A. and Lagnado, D. A.},
	booktitle = {{Proceedings of the 33rd Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Carlson, C. and H{\"o}lscher, C. and Shipley, T.},
	pages = {720--725},
	publisher = {{Austin, TX: Cognitive Science Society}},
	title = {Blame the skilled},
	year = {2011}}

@inproceedings{schaechtele2011beyond,
	abstract = {To what extent do people care about the intentions behind an action? What if the intentions can be deceptive? We conducted two experiments to complement previous evidence about the roles of outcomes and intentions in economic games. The results of Experiment 1 indicate that both outcomes and intentions affect players' responses. Moreover, unkind intentions are punished but kind intentions are hardly rewarded. In Experiment 2, intentions are stated as opposed to observed. Participants misstate their intentions frequently, thereby undermining the credibility of the statements. As a result, perceived honesty modulates players' responses.},
	address = {Austin, TX},
	author = {Sch\"achtele, S. and Gerstenberg, T. and Lagnado, D. A.},
	booktitle = {{Proceedings of the 33rd Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {L. Carlson and C. H\"olscher and T. Shipley},
	pages = {1860--1865},
	publisher = {Cognitive Science Society},
	title = {Beyond outcomes: The influence of intentions and deception},
	year = {2011}}

@inproceedings{gerstenberg2013back,
	abstract = {Would Dan have died if Bob hadn't shot? In this paper, we show that people's answer depends on whether or not they are asked about what would have caused Bob not to shoot. Something needs to change in order to turn an actual world into a counterfactual world. Previous findings of how people reason about counterfactuals have been mixed: sometimes people appear to backtrack and reevaluate the causes of a counterfactual state (e.g. Rips, 2010). At other times, people appear to treat counterfactuals like interventions that leave the past unchanged (Sloman & Lagnado, 2005). We experimentally manipulated the order in which participants were asked to consider the consequences of a counterfactual state. The results show that participants are more likely to backtrack when explicitly asked to consider a counterfactual's causes. However, when directly asked about the effects of a counterfactual state, most people don't backtrack.},
	address = {Austin, TX},
	author = {Gerstenberg, T. and Bechlivanidis, C. and Lagnado, D. A.},
	booktitle = {{Proceedings of the 35th Annual Conference of the Cognitive Science Society}},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {M. Knauff and M. Pauen and N. Sebanz and I. Wachsmuth},
	pages = {2386--2391},
	publisher = {Cognitive Science Society},
	title = {Back on track: Backtracking in counterfactual reasoning},
	year = {2013}}

@incollection{gerstenberg2014attributing,
	abstract = {How do people attribute responsibility to individuals in a group? Several models in psychology predict that there is a close relationship between counterfactuals and responsibility: how responsible an individual's contribution is seen depends on whether it made a difference to the outcome. We first review these models and then point out a major limitation: people sometimes hold individuals responsible even though their contribution made no relevant difference to the outcome. A richer conception of the relationship between counterfactuals and responsibility is necessary. People's attributions of responsibility are not only influenced by whether a person's contribution made a difference in the actual situation, but also by whether it would have made a difference in other possible situations. We propose a general framework that conceptualizes attributions of responsibility in terms of counterfactuals defined over structured causal models. Using this framework, we show that retrospective responsibility attributions are also affected by prospective responsibility. A person's responsibility depends on how critical their contribution was perceived for the group's success and on how close it was to making a difference to the outcome.},
	author = {Gerstenberg, T. and Lagnado, D. A.},
	booktitle = {Oxford Studies in Experimental Philosophy},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {Joshua Knobe and Tania Lombrozo and Shaun Nichols},
	pages = {91--130},
	publisher = {Oxford University Press},
	title = {Attributing responsibility: Actual and counterfactual worlds},
	volume = {1},
	year = {2014}}

@incollection{lagnado2015difference,
	author = {David A. Lagnado and Tobias Gerstenberg},
	booktitle = {Oxford Studies in Agency and Responsibility},
	date-added = {2017-11-18 19:07:12 +0000},
	date-modified = {2017-11-18 19:07:12 +0000},
	editor = {David Shoemaker},
	pages = {213--241},
	publisher = {Oxford University Press},
	title = {A difference-making framework for intuitive judgments of responsibility},
	volume = {3},
	year = {2015}}
