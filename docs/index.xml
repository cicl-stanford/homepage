<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Causality in Cognition Lab on Causality in Cognition Lab</title>
    <link>https://cicl.stanford.edu/</link>
    <description>Recent content in Causality in Cognition Lab on Causality in Cognition Lab</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2024 Tobias Gerstenberg</copyright>
    <lastBuildDate>Tue, 14 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>From Artifacts to Human Lives: Investigating the Domain-Generality of Judgments about Purposes</title>
      <link>https://cicl.stanford.edu/publication/prinzing2025purpose/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/prinzing2025purpose/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A signaling theory of self-handicapping</title>
      <link>https://cicl.stanford.edu/publication/xiang2024handicapping/</link>
      <pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/xiang2024handicapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Imagining and building wise machines: The centrality of AI metacognition</title>
      <link>https://cicl.stanford.edu/publication/johnson2024wise/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/johnson2024wise/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-supervised alignment with mutual information: Learning to follow principles without preference labels</title>
      <link>https://cicl.stanford.edu/publication/franken2024sami/</link>
      <pubDate>Tue, 08 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/franken2024sami/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MARPLE: A Benchmark for Long-Horizon Inference</title>
      <link>https://cicl.stanford.edu/publication/jin2024marple/</link>
      <pubDate>Fri, 04 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/jin2024marple/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Causation, Meaning, and Communication</title>
      <link>https://cicl.stanford.edu/publication/beller2024causation/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/beller2024causation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human-like Affective Cognition in Foundation Models</title>
      <link>https://cicl.stanford.edu/publication/gandhi2024affective/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/gandhi2024affective/</guid>
      <description></description>
    </item>
    
    <item>
      <title>To Err is Robotic: Rapid Value-Based Trial-and-Error during Deployment</title>
      <link>https://cicl.stanford.edu/publication/du2024robotic/</link>
      <pubDate>Wed, 26 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/du2024robotic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Counterfactual simulation in causal cognition</title>
      <link>https://cicl.stanford.edu/publication/gerstenberg2024counterfactual/</link>
      <pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/gerstenberg2024counterfactual/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Children use disagreement to infer what happened</title>
      <link>https://cicl.stanford.edu/publication/amemiya2024disagreement/</link>
      <pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/amemiya2024disagreement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Whodunnit? Inferring what happened from multimodal evidence</title>
      <link>https://cicl.stanford.edu/publication/wu2024whodunnit/</link>
      <pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/wu2024whodunnit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards a computational model of responsibility judgments in sequential human-AI collaboration</title>
      <link>https://cicl.stanford.edu/publication/tsirtsis2024sequential/</link>
      <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/tsirtsis2024sequential/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Without his cookies, he&#39;s just a monster: A counterfactual simulation model of social explanation</title>
      <link>https://cicl.stanford.edu/publication/brockbank2024monster/</link>
      <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/brockbank2024monster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Chain versus common cause: Biased causal strength judgments in humans and large language models</title>
      <link>https://cicl.stanford.edu/publication/keshmirian2024chain/</link>
      <pubDate>Sat, 11 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/keshmirian2024chain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Do as I explain: Explanations communicate optimal interventions</title>
      <link>https://cicl.stanford.edu/publication/kirfel2024do/</link>
      <pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cicl.stanford.edu/publication/kirfel2024do/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
